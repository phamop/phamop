High-Level Flow details
------------------------------
AKS Applications generate telemetry (instrumented via OpenTelemetry SDKs, Opentelemetry annotations or sidecars).

 OpenTelemetry Auto instrumentation  push data VM’s OTel Collector  Endpoints

VM OTel Collector distributes data to the respective exporter in the vm:

Metrics → Prometheus (scraped or pushed)

Logs → Loki (HTTP push)

Traces → Jaeger (gRPC push)

Grafana queries all backends (Loki, Prometheus, Jaeger) for visualization.










AKS (Workloads + OTel Agent/Sidecars)
   │ OTLP:gRPC/HTTP (4317/4318)
   ▼
Azure Standard Internal Load Balancer (ILB, single VIP)
   ├─ Health probe: 4317 (OTLP gRPC), 4318 (HTTP)
   ▼
Azure VMSS (min=3, max=10) – each VM runs Docker:
   • nginx (TLS/route UI if not using AppGW)
   • otel-collector (gateway)  ← scrapes to Prom exporter :9464
   • prometheus  + thanos-sidecar (local TSDB on Premium SSD)
   • thanos-store + thanos-query (across all shards)
   • loki (distributor/ingester/querier/query-frontend/compactor)
   • jaeger-collector + jaeger-query (UI)
   • grafana (SSO via Entra ID; uses shared Postgres DB)
Storage:
   • Premium SSD (/data):
       - /data/prometheus, /data/loki-cache, /data/thanos-cache
   • Azure Blob:
       - thanos-bucket (metrics, immutable blocks)
       - loki-bucket (chunks + index via boltdb-shipper)
   • Traces: Elastic Cloud on Azure (recommended) or OpenSearch VMSS
UIs:
   • App Gateway (WAF) → nginx (or direct) → Grafana/Jaeger
Security:
   • NSGs (strict), TLS/mTLS, Key Vault, Managed Identity



Receive all metrics, logs, traces from AKS via OTLP.

Run Grafana, Prometheus+Thanos, Loki (distributed), Jaeger (collectors/query) and an OTel gateway on every VM (Docker).

Use Premium SSD for fast local TSDB/WAL/caches; use Azure Blob for durable, scalable object storage.

Scale from 3 → 10 VMs without client reconfiguration.

Provide TLS/mTLS, SSO, least-privilege access, and safe concurrency (no data corruption).




Access the Azure pricing estimate for this solution via the calculator link below:



1. Compute Costs (VM)
VM Size	vCPUs	RAM	Cost (CAD/month)	Use Case
Standard_D8s_v3	8	32GB	~$292.80	Medium workload (Dev/Test)
Standard_D16s_v3	16	64GB	~$585.60	Large workload (Production)
Notes:


2. Storage Costs (Premium SSD)
Disk Type	Size	Cost (CAD/month)	Purpose
OS Disk (P10)	128GB	~$22.40	Boot volume
Data Disk (P20)	512GB	~$89.60	Loki (logs)
Data Disk (P15)	256GB	~$44.80	Prometheus (metrics)
Data Disk (P15)	256GB	~$44.80	Jaeger (traces)
Data Disk (P10)	128GB	~$22.40	Grafana (dashboards)
Total Storage	1.25TB	~$224.00	
Notes:

Premium SSD (P10/P15/P20) is recommended for high IOPS.

Standard SSD could reduce costs by ~30% but may impact performance.




3. Network Traffic Costs
Traffic Type	Data Volume	Cost (CAD/month)
Inbound Data	Free	$0.00
Outbound Data	First 5GB	$0.00
Additional Outbound	$0.087/GB	~$8.70 (100GB est.)
Assumptions:

100GB/month outbound (AKS → VM telemetry).

Inbound traffic is always free in Azure.



 . Total Estimated Monthly Cost
Component	Medium VM (D8s_v3)	Large VM (D16s_v3)
Compute	$292.80	$585.60
Storage	$224.00	$224.00
Network	$8.70	$8.70
Total	~$525.50	~$818.30

https://azure.microsoft.com/en-ca/pricing/calculator/


Cost Optimization Tips
Reserved Instances (1- or 3-year commitment)

Save up to 72% on compute costs (e.g., D8s_v3 drops to ~$82/month).

Azure Hybrid Benefit

Use existing Linux licenses to reduce costs by ~40%.

Storage Tiering

Move older logs/metrics to Cool Blob Storage (~$0.02/GB/month).

Auto-Shutdown for Dev/Test

Shut down VMs nights/weekends to save ~50% on compute.



we’ll need estimated actual costs; please add a table with the costs per/month for compute, storage and traffic; use the azure price calculator to get pricing, Canada Central Region


## Title  
Dev: Implement Temporary VM-hosted OSS Observability Stack

## 1. Description  
- **Problem/Need:**  
  Implement the approved design to deploy a temporary, private VM-hosted OSS observability stack (Grafana, Prometheus, Loki, Jaeger) and wire AKS telemetry via OpenTelemetry Collector, without changing AKS/APIM ingress.

- **Expected Outcome:**  
  Working deployment on a Linux VM using Docker Compose and Ansible, with existing network-only access, short retention, and a brief runbook. Telemetry from AKS is visible end-to-end.

- **Related Links:**  
  - (Link to the approved design story)

## 2. Implementation Approach  
- **Key Steps:**  
  - Provision VM in the existing subnet; apply firewall/NSG rules for local-only access.  
  - Use Ansible to install Docker, deploy Docker Compose stack (Grafana, Prometheus with remote write receiver, Loki, Jaeger), configure restart policies.  
  - Configure OpenTelemetry Collector in AKS to forward metrics (remote write), logs (Loki HTTP), and traces (OTLP) to the VM.  
  - Apply credentials, RBAC, and disable anonymous access; set retention in Prometheus/Loki per design.  
  - Validate telemetry flow and UI access; create a short runbook for common tasks and troubleshooting.  
  - Version compose files and Ansible playbooks in the repo; record decommission/sunset steps.

## 3. Value & Priority  
- **Impact:**  
  Restores developer observability rapidly with minimal risk and clear rollback path.

























Apps & PaaS → Opentelemetry AutoInstrumentation(Optional)  → VM OTel Collector (gateway)
    → Prometheus (metrics) → Grafana
    → Loki (logs)          → Grafana
    → Jaeger (traces)      → Grafana
Components
Azure VM (Obs Gateway & Backends):
OTel Collector (gateway)
Prometheus (scrape & remote_write endpoints)
Loki (log ingestion)
Jaeger (collector + query + UI)
Grafana (dashboards/alerts)
In Cluster: OpenTelemetry AutoInstrumentation CRD ( Autoinstrumentation injection for node/app scraping/export).



















What We Install on the VM
The dedicated VM runs the entire observability stack via Docker:
Installed Components (Docker Containers)
Component	Role	Data Type	Port
OpenTelemetry Collector	Central telemetry receiver	Logs/Metrics/Traces	4317, 4318
Loki	Log aggregation & storage	Logs	3100
Prometheus	Metrics scraping & storage	Metrics	9090
Jaeger	Distributed tracing	Traces	14268, 16686
Grafana	Dashboards & visualization	UI	3000
Configuration Files
•	otel-collector-config.yaml (Routes telemetry to Loki/Prometheus/Jaeger)
•	prometheus.yml (Defines scrape targets in AKS)
•	loki-config.yaml (Log retention, storage settings)
•	grafana-datasources.yaml (Pre-configures Loki/Prometheus as data sources)


High-Level Flow details
------------------------------
AKS Applications generate telemetry (instrumented via OpenTelemetry SDKs or sidecars).

OTel Collector (Sidecar in AKS) collects and forwards data to the VM’s OTel Collector.

VM OTel Collector distributes data:

Metrics → Prometheus (scraped or pushed)

Logs → Loki (HTTP push)

Traces → Jaeger (gRPC push)

Grafana queries all backends (Loki, Prometheus, Jaeger) for visualization.























o	How data is pulled or pushed between the VM and AKS
o	What exactly do we install on the VM and AKS
o	Visualize the network planes in those data flows to ensure things like firewalls nsg's etc. don't become hurdles during implementation
o	How do we secure the flow (mTLS, authentication, etc.)

Environmental constraints have redirected OSS observability priorities, leading to the closure of this story



error: Error: failed to perform "FetchReference" on source: from ACR after it confirned it has been pushed to the ACR

apiVersion: v2
name: prom-operator-crds-bootstrap
description: Installs Prometheus Operator CRDs (v0.83.0) via dependency chart
type: application
version: 1.0.0
appVersion: "0.83.0"

dependencies:
  - name: prometheus-operator-crds
    alias: promOperatorCRDs
    version: 22.0.2
    repository: oci://private-registry.azurecr.io/
    condition: promOperatorCRDs.enabled

# values.yaml
promOperatorCRDs:
  enabled: true






#source.yaml
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
  name: prometheus-community
  labels:
     repoName: prometheus
#  namespace: monitoring
spec:
  url: https://prometheus-community.github.io/helm-charts
  interval: 1h


# url: https://prometheus-community.github.io/helm-charts
# repoName: prometheus

#chart.yaml 

---
apiVersion: v2
name: prometheus
description: Prometheus metrics data source for observability
type: application
version: 1.0.0
appVersion: "1.0.0"

dependencies:
  - name: prometheus
    version: 27.23.0
    repository: oci://private-registry.azurecr.io/hbt/charts
  - name: alertmanager
    version: 1.22.0
    repository: oci://private-registry.azurecr.io/hbt/charts

---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: prom-operator-crds
#  namespace: monitoring
spec:
  interval: 30m
  chart:
    spec:
      chart: prometheus-operator-crds
      version: 22.0.2          
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
#        namespace: monitoring
  install:
    crds: Create
  upgrade:
    crds: CreateReplace


