grafana:
  env:
    GF_SERVER_ROOT_URL: https://eddv3-hbt/grafana
    GF_SERVER_SERVE_FROM_SUB_PATH: "true"



logger=context userId=0 orgId=0 uname= t=2025-07-31T20:35:37.082770955Z level=info msg="Request Completed" method=GET path=/ status=302 remote_addr=10.255.192.133 time_ms=311 duration=311.422512ms size=29 referer= handler=/ status_source=server
logger=context userId=0 orgId=0 uname= t=2025-07-31T20:36:11.971326789Z level=info msg="Request Completed" method=GET path=/ status=302 remote_addr=10.255.192.133 time_ms=82 duration=82.102194ms size=29 referer= handler=/ status_source=server
logger=context userId=0 orgId=0 uname= t=2025-07-31T20:36:17.262081381Z level=info msg="Request Completed" method=GET path=/ status=302 remote_addr=10.255.192.133 time_ms=0 duration=155.601µs size=29 referer= handler=/ status_source=server
logger=cleanup t=2025-07-31T20:43:46.19951493Z level=info msg="Completed cleanup jobs" duration=519.902481ms
logger=plugins.update.checker t=2025-07-31T20:43:47.438978231Z level=info msg="Update check succeeded" duration=148.330601ms
logger=context userId=0 orgId=0 uname= t=2025-07-31T20:44:18.58904824Z level=info msg="Request Completed" method=GET path=/ status=302 remote_addr=10.244.4.1 time_ms=118 duration=118.442097ms size=29 referer= handler=/ status_source=server




10.244.4.1 - - [31/Jul/2025:20:21:39 +0000] "GET /grafana/ HTTP/1.1" 301 104 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36" 812 0.010 [eddv3-hbt-grafana-80] [] 10.244.4.230:3000 104 0.010 301 0b1394facf20c10bc46ad1e50d9dbb54
10.255.192.133 - - [31/Jul/2025:20:21:39 +0000] "GET /grafana/ HTTP/1.1" 301 104 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36" 812 0.007 [eddv3-hbt-grafana-80] [] 10.244.4.230:3000 104 0.007 301 15f4780d7d87b3b1004b75bc5a6871b2
10.255.192.132 - - [31/Jul/2025:20:21:39 +0000] "GET /grafana/ HTTP/1.1" 301 104 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36" 812 0.002 [eddv3-hbt-grafana-80] [] 10.244.4.230:3000 104 0.002 301 51de5e39e2fe6e05abd71a1ef6dfb32b
10.255.192.135 - - [31/Jul/2025:20:21:39 +0000] "GET /grafana/ HTTP/1.1" 301 104 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36" 812 0.002 [eddv3-hbt-grafana-80] [] 10.244.4.230:3000 104 0.002 301 e1149cfea4f35e48fd1b62d74a618f40
10.244.4.1 - - [31/Jul/2025:20:21:39 +0000] "GET /grafana/ HTTP/1.1" 301 104 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36" 812 0.024 [eddv3-hbt-grafana-80] [] 10.244.4.230:3000 104 0.024 301 d72726afe599cf15957689c1c33dff0f
10.255.192.133 - - [31/Jul/2025:20:21:39 +0000] "GET /grafana/ HTTP/1.1" 301 104 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36" 812 0.021 [eddv3-hbt-grafana-80] [] 10.244.4.230:3000 104 0.021 301 922fd25594cafc1d784c5a8b6f20bf07




 - - [31/Jul/2025:16:23:35 +0000] "GET /grafana/ HTTP/1.1" 301 103 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36" 812 0.004 [eddv3-hbt-grafana-80] [] 10.244.4.48:3000 103 0.004 301 b490c882acc8342a3e2153c06282cf32
 - - [31/Jul/2025:16:23:35 +0000] "GET /grafana/ HTTP/1.1" 301 103 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36" 812 0.002 [eddv3-hbt-grafana-80] [] 10.244.4.48:3000 103 0.002 301 0c8e8edfbd7726a89cd5524e2ff048da


Error: template: grafana-proxy/templates/service.yaml:4:11: executing "grafana-proxy/templates/service.yaml" at <include "grafana-proxy.fullname" .>: error calling include: template: no template "grafana-proxy.fullname" associated with template "gotpl"
Release "grafana-proxy" does not exist. Installing it now.
##[error]Error: template: grafana-proxy/templates/service.yaml:4:11: executing "grafana-proxy/templates/service.yaml" at <include "grafana-proxy.fullname" .>: error calling include: template: no template "grafana-proxy.fullname" associated with template "gotpl"


 
{{/*
Return the full name of the release
*/}}
{{- define "grafana-proxy.fullname" -}}
{{- if .Values.nameOverride }}
{{- .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name "proxy" | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}

 
 
 
 
 
 "WARNING", "msg": "Retrying (Retry(total=4, connect=9, read=5, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f5620202e90>: Failed to establish a new connection: [Errno 111] Connection refused')': /api/v1/namespaces/good9009/configmaps?labelSelector=loki_rule&timeoutSeconds=60&watch=True"}


mode: deployment  # Changed from daemonset to deployment

# Basic configuration
nameOverride: "my-opentelemetry-collector"
fullnameOverride: "my-opentelemetry-collector"

replicaCount: 2  # Number of replicas for the deployment

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
        http:
          endpoint: "0.0.0.0:4318"

  processors:
    batch: {}
    memory_limiter:
      check_interval: 1s
      limit_mib: 400
      spike_limit_mib: 100

  exporters:
    logging:
      verbosity: detailed
    otlphttp:
      endpoint: "http://loki-gateway.good9009.svc.cluster.local:3100/otlp"

  service:
    pipelines:
      traces:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [logging, otlphttp]
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [logging, otlphttp]
      logs:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [logging, otlphttp]

image:
  repository: otel/opentelemetry-collector-contrib
  tag: 0.123.0
  pullPolicy: IfNotPresent

resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 200m
    memory: 256Mi

service:
  type: ClusterIP
  name: my-opentelemetry-collector  # Explicit service name
  ports:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
    - name: otlp-http
      port: 4318
      targetPort: 4318
    - name: metrics
      port: 8888
      targetPort: 8888

ports:
  otlp:
    enabled: true
    containerPort: 4317
  otlp-http:
    enabled: true
    containerPort: 4318
  metrics:
    enabled: true
    containerPort: 8888

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 60
  targetMemoryUtilizationPercentage: 60



#####################
# values.yaml
mode: deployment  # Changed from daemonset to deployment

replicaCount: 3  # Number of replicas for the deployment

# Disable presets not needed in deployment mode
presets:
  logsCollection:
    enabled: false
  hostMetrics:
    enabled: false
  kubernetesAttributes:
    enabled: true  # Keep this to enrich metrics with k8s metadata
  kubeletMetrics:
    enabled: false
  kubernetesEvents:
    enabled: false
  clusterMetrics:
    enabled: false

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
        http:
          endpoint: "0.0.0.0:4318"
  
  processors:
    batch: {}
    memory_limiter:
      check_interval: 1s
      limit_percentage: 75
      spike_limit_percentage: 15
  
  exporters:
    logging:
      verbosity: detailed
    otlphttp:
      endpoint: "http://your-backend:4318"  # Replace with your actual backend
  
  extensions:
    health_check:
      endpoint: "0.0.0.0:13133"
    zpages:
      endpoint: "0.0.0.0:55679"
  
  service:
    telemetry:
      logs:
        level: info
    extensions: [health_check, zpages]
    pipelines:
      traces:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [logging, otlphttp]
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [logging, otlphttp]
      logs:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [logging, otlphttp]

image:
  repository: otel/opentelemetry-collector-contrib
  tag: 0.96.0  # Using a recent stable version
  pullPolicy: IfNotPresent

resources:
  requests:
    cpu: 200m
    memory: 512Mi
  limits:
    cpu: 1000m
    memory: 2Gi

serviceAccount:
  create: true

# Port configurations
ports:
  otlp-grpc:
    enabled: true
    containerPort: 4317
    servicePort: 4317
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888

# Probes using the health check extension
livenessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 15
  timeoutSeconds: 5

readinessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 5
  timeoutSeconds: 3

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 60
  targetMemoryUtilizationPercentage: 60

service:
  type: ClusterIP
  annotations: {}
  # If you need NodePort for external access:
  # type: NodePort
  # nodePorts:
  #   otlp-grpc: 30001
  #   otlp-http: 30002

# Optional: Ingress for HTTP endpoints
ingress:
  enabled: false
  # className: nginx
  # annotations:
  #   cert-manager.io/cluster-issuer: letsencrypt-prod
  # hosts:
  #   - host: otel.example.com
  #     paths:
  #       - path: /
  #         pathType: Prefix
  # tls:
  #   - secretName: otel-tls
  #     hosts:
  #       - otel.example.com






https://jessitron.com/2023/09/08/testing-an-opentelemetry-collector-deployed-as-a-daemonset-in-kubernetes/

https://opentelemetry.io/docs/collector/configuration/

opentelemetry-collector:
- service: Additional property labels is not allowed
- presets.logsCollection: Additional property maxRecombineLogsSize is not allowed



# Fixed OpenTelemetry Collector DaemonSet and Service Configuration

Here's the corrected and properly formatted YAML configuration for deploying the OpenTelemetry Collector in DaemonSet mode with a corresponding Service:

```yaml
mode: daemonset

presets:
  logsCollection:
    enabled: true
    includeCollectorLogs: false
    # Removed maxRecombineLogsSize as it's not in schema
  hostMetrics:
    enabled: true
  kubernetesAttributes:
    enabled: true
  kubeletMetrics:
    enabled: true
  kubernetesEvents:
    enabled: false
  clusterMetrics:
    enabled: false

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
        http:
          endpoint: "0.0.0.0:4318"
  
  processors:
    transform:
      error_mode: ignore
      log_statements:
        - context: log
          statements:
            - set(resource.attributes["service.name"], attributes["service.name"])
            - delete_key(attributes, "service.name")
  
  exporters:
    debug:
      verbosity: detailed
    otlphttp:
      endpoint: http://loki-gateway.good9009.svc.cluster.local:3100/otlp
  
  extensions:
    health_check:
      endpoint: 0.0.0.0:13133
  
  service:
    telemetry:
      logs:
        level: debug
    extensions:
      - health_check
    pipelines:
      logs:
        receivers: [otlp]
        processors: [transform]
        exporters: [debug, otlphttp]

image:
  repository: otel/opentelemetry-collector-contrib
  tag: 0.123.0
  pullPolicy: IfNotPresent

resources:
  requests:
    cpu: 200m
    memory: 260Mi
  limits:
    cpu: 1000m
    memory: 1000Mi

serviceAccount:
  create: true

ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    hostPort: 4318
  jaeger-compact:
    enabled: true
    containerPort: 6831
    hostPort: 6831
    protocol: UDP
  jaeger-thrift:
    enabled: true
    containerPort: 14268
    servicePort: 14268
    hostPort: 14268
  jaeger-grpc:
    enabled: true
    containerPort: 14250
    servicePort: 14250
    hostPort: 14250
  zipkin:
    enabled: true
    containerPort: 9411
    servicePort: 9411
    hostPort: 9411
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    hostPort: 8888
  prometheus:
    enabled: true
    containerPort: 8889
    servicePort: 8889

livenessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 15
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3

# Minimal service configuration
service:
  type: ClusterIP
  # Removed labels as they're not in schema
  # Removed annotations as they're not in schema












#################################

Engaged with stakeholders on upgrade plans.
Followed up with UNIX team for pending requirements.
Qlik PostgreSQL 16 DB instance setup with DBAs in progress
Completed requested GitLab tasks.
Coordinated GitLab EOL (RHEL7 → RHEL9) and received approval for new server deployment.
Submit new server request once project name is confirmed.
Continue stakeholder syncs on GitLab RHEL9 migration.
Ensured Artifactory and Xray were operational.
Xray upgrade paused due to storage constraints 
Revisit Xray upgrade after storage resolution.
Assisted colleagues with application support tasks.






# Azure Monitor Insights Integration with Grafana on AKS: Implementation Guide


https://learn.microsoft.com/en-us/azure/aks/use-azure-ad-pod-identity
https://learn.microsoft.com/en-us/azure/aks/use-azure-ad-pod-identity#access-azure-key-vault-using-managed-identity
https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview?tabs=dotnet




Could you confirm if you are experiencing any performance impact on the server following the recent upgrade?
Response: Yes, we are observing slowness.

Could you provide additional details about the upgrade task performed?
Please refer to the attached yum-note.txt file for details.
Key Notes:

The upgrade started at 10:22 AM (EST) and completed by 2:11 PM (EST).

At 11:09 AM (EST) (log line 33, timestamp 2025-04-21 15:09:26 UTC), the process was still ongoing.

The next recorded update was at 1:58 PM (EST) (2025-04-21T13:58:22-04:00).

Based on the timestamps, the backup alone took approximately 2 hours and 40 minutes.
Could you provide technical clarification on this duration?

What is the current status of the server?
The server is operational but continues to exhibit sluggish performance.

Were any changes made to the server prior to this issue?
No other modifications were made—the slowness began after the upgrade.

Please let us know if further details are required.

This guide provides a detailed implementation plan for integrating Azure Monitor Insights metrics with Grafana on an existing AKS cluster, using Azure Key Vault for secrets management and Azure DevOps pipelines for deployment.

## Prerequisites

- Existing AKS cluster
- Existing Azure Monitor workspace
- Existing Azure Key Vault (Goodvault)
- Azure DevOps organization
- Helm installed in your local environment

## Architecture Overview

1. Grafana deployed via Helm to AKS
2. Azure Monitor Metrics integration configured in Grafana
3. Secrets managed via Azure Key Vault
4. Separate Azure DevOps pipelines for:
   - Grafana deployment
   - Azure Monitor integration
5. Environment-specific configuration via parameter files

## Implementation Steps

### 1. Prepare Helm Charts

**Grafana Helm Chart Structure:**
```
grafana/
├── Chart.yaml
├── values.yaml
├── values-dev.yaml
├── values-prod.yaml
├── templates/
│   ├── deployment.yaml
│   ├── service.yaml
│   ├── configmap.yaml
│   ├── secrets.yaml
│   └── ingress.yaml
└── crds/
    └── grafana-dashboards.yaml
```

### 2. Create Custom Resource Definitions (CRDs)

**grafana/crds/grafana-dashboards.yaml:**
```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: grafanadashboards.integrations.grafana.com
spec:
  group: integrations.grafana.com
  names:
    kind: GrafanaDashboard
    listKind: GrafanaDashboardList
    plural: grafanadashboards
    singular: grafanadashboard
  scope: Namespaced
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              x-kubernetes-preserve-unknown-fields: true
```

### 3. Configure Azure Monitor Data Source

Create a configuration file for Azure Monitor data source:

**grafana/templates/azure-monitor-datasource.yaml:**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-azure-monitor-datasource
  labels:
    app: grafana
data:
  azure-monitor-datasource.yaml: |
    apiVersion: 1
    datasources:
    - name: AzureMonitor
      type: grafana-azure-monitor-datasource
      access: proxy
      jsonData:
        subscriptionId: ${SUBSCRIPTION_ID}
        cloudName: azuremonitor
        azureAuthType: msi
        tenantId: ${TENANT_ID}
        clientId: ${CLIENT_ID}
      secureJsonData:
        clientSecret: ${CLIENT_SECRET}
      editable: true
```

### 4. Set Up Azure Key Vault Integration

**Store these secrets in Goodvault:**
- Grafana admin username
- Grafana admin password
- Azure Service Principal credentials (for Azure Monitor integration)

### 5. Create Azure DevOps Pipelines

#### Pipeline 1: Grafana Deployment Pipeline

**azure-pipelines-grafana-deploy.yml:**
```yaml
trigger:
  branches:
    include:
    - main
  paths:
    include:
    - grafana/*

variables:
  - group: Global-Kubernetes-Config
  - name: environment
    value: 'dev'
  - name: helmReleaseName
    value: 'grafana'

stages:
- stage: Deploy
  jobs:
  - job: Deploy_Grafana
    steps:
    - task: HelmInstaller@1
      displayName: 'Install Helm'
      inputs:
        helmVersionToInstall: 'latest'
    
    - task: AzureKeyVault@2
      displayName: 'Get Secrets from Key Vault'
      inputs:
        azureSubscription: '$(azureServiceConnection)'
        KeyVaultName: 'Goodvault'
        SecretsFilter: '*'
    
    - task: Kubernetes@1
      displayName: 'Create Kubernetes namespace'
      inputs:
        connectionType: 'Azure Resource Manager'
        azureSubscriptionEndpoint: '$(azureServiceConnection)'
        azureResourceGroup: '$(resourceGroup)'
        kubernetesCluster: '$(aksClusterName)'
        command: 'apply'
        arguments: '-f grafana-namespace.yaml'
        secretType: 'dockerRegistry'
    
    - task: HelmDeploy@0
      displayName: 'Deploy Grafana with Helm'
      inputs:
        connectionType: 'Azure Resource Manager'
        azureSubscriptionEndpoint: '$(azureServiceConnection)'
        azureResourceGroup: '$(resourceGroup)'
        kubernetesCluster: '$(aksClusterName)'
        namespace: 'grafana'
        command: 'upgrade'
        chartType: 'FilePath'
        chartPath: 'grafana'
        releaseName: '$(helmReleaseName)'
        valueFile: 'grafana/values-$(environment).yaml'
        overrideValues: 'grafana.admin.user=$(GRAFANA_ADMIN_USER),grafana.admin.password=$(GRAFANA_ADMIN_PASSWORD)'
        install: true
```

#### Pipeline 2: Azure Monitor Integration Pipeline

**azure-pipelines-azuremonitor-integration.yml:**
```yaml
trigger:
  branches:
    include:
    - main
  paths:
    include:
    - azure-monitor-integration/*

variables:
  - group: Global-Kubernetes-Config
  - name: environment
    value: 'dev'

stages:
- stage: Integrate
  jobs:
  - job: Configure_AzureMonitor
    steps:
    - task: AzureKeyVault@2
      displayName: 'Get Azure Monitor Secrets'
      inputs:
        azureSubscription: '$(azureServiceConnection)'
        KeyVaultName: 'Goodvault'
        SecretsFilter: 'AZURE-CLIENT-ID, AZURE-CLIENT-SECRET, AZURE-TENANT-ID'
    
    - task: Kubernetes@1
      displayName: 'Apply Azure Monitor Config'
      inputs:
        connectionType: 'Azure Resource Manager'
        azureSubscriptionEndpoint: '$(azureServiceConnection)'
        azureResourceGroup: '$(resourceGroup)'
        kubernetesCluster: '$(aksClusterName)'
        namespace: 'grafana'
        command: 'apply'
        arguments: '-f azure-monitor-integration/azure-monitor-datasource.yaml'
        secretType: 'generic'
        secretArguments: '--from-literal=SUBSCRIPTION_ID=$(AZURE-SUBSCRIPTION-ID) --from-literal=TENANT_ID=$(AZURE-TENANT-ID) --from-literal=CLIENT_ID=$(AZURE-CLIENT-ID) --from-literal=CLIENT_SECRET=$(AZURE-CLIENT-SECRET)'
```

### 6. Parameterize Configuration Files

**values-dev.yaml:**
```yaml
replicaCount: 1
image:
  repository: grafana/grafana
  tag: 8.3.4
  pullPolicy: IfNotPresent

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - grafana-dev.yourdomain.com
  tls:
    - secretName: grafana-dev-tls
      hosts:
        - grafana-dev.yourdomain.com

persistence:
  enabled: true
  storageClassName: managed-premium
  accessModes:
    - ReadWriteOnce
  size: 10Gi

grafana.ini:
  paths:
    data: /var/lib/grafana/data
    logs: /var/log/grafana
    plugins: /var/lib/grafana/plugins
    provisioning: /etc/grafana/provisioning
  analytics:
    check_for_updates: true
  log:
    mode: console
  grafana_net:
    url: https://grafana.net
```

**values-prod.yaml:**
```yaml
replicaCount: 3
image:
  repository: grafana/grafana
  tag: 8.3.4
  pullPolicy: Always

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - grafana.yourdomain.com
  tls:
    - secretName: grafana-tls
      hosts:
        - grafana.yourdomain.com

persistence:
  enabled: true
  storageClassName: managed-premium
  accessModes:
    - ReadWriteOnce
  size: 50Gi

grafana.ini:
  paths:
    data: /var/lib/grafana/data
    logs: /var/log/grafana
    plugins: /var/lib/grafana/plugins
    provisioning: /etc/grafana/provisioning
  analytics:
    check_for_updates: true
  log:
    mode: console
  grafana_net:
    url: https://grafana.net
```

### 7. Create Variable Groups in Azure DevOps

1. Navigate to Azure DevOps > Pipelines > Library
2. Create a variable group named "Global-Kubernetes-Config" with:
   - aksClusterName
   - resourceGroup
   - azureServiceConnection (service connection name)
3. Create environment-specific variable groups (e.g., "Dev-Config", "Prod-Config") with:
   - GRAFANA_ADMIN_USER
   - GRAFANA_ADMIN_PASSWORD (mark as secret)
   - AZURE-SUBSCRIPTION-ID
   - AZURE-TENANT-ID
   - AZURE-CLIENT-ID
   - AZURE-CLIENT-SECRET (mark as secret)

### 8. Deployment Process

1. **Initial Deployment:**
   - Run the Grafana deployment pipeline first
   - This creates the namespace, deploys Grafana, and sets up basic configuration

2. **Azure Monitor Integration:**
   - Run the Azure Monitor integration pipeline
   - This configures the data source and connects Grafana to Azure Monitor metrics

3. **Verification:**
   - Access Grafana via the ingress URL
   - Verify Azure Monitor data source is configured
   - Create test dashboards using Azure Monitor metrics

## Maintenance and Updates

1. **Updating Grafana:**
   - Update the image tag in values.yaml
   - Pipeline will automatically apply changes on next run

2. **Adding New Dashboards:**
   - Create new CRD yaml files in the crds directory
   - Add them to the helm chart
   - Pipeline will apply them on next run

3. **Rotating Secrets:**
   - Update secrets in Azure Key Vault
   - Re-run pipelines to apply updated secrets

## Security Considerations

1. All secrets are stored in Azure Key Vault
2. Kubernetes secrets are created dynamically during deployment
3. RBAC is enforced at the AKS level
4. Grafana admin credentials are rotated regularly
5. Network policies restrict access to Grafana pods

This implementation provides a secure, scalable, and maintainable solution for integrating Azure Monitor Insights with Grafana on AKS, with proper separation of concerns between deployment and integration pipelines, and environment-specific configuration management.
