service/otel-collector unchanged
servicemonitor.monitoring.coreos.com/otel-collector-monitor unchanged
Error from server: error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"opentelemetry.io/v1beta1\",\"kind\":\"OpenTelemetryCollector\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"opentelemetry-collector\",\"hbt.cmhc.schl.gc.ca/owner\":\"devops\"},\"name\":\"otel-collector\",\"namespace\":\"monitoring\"},\"spec\":{\"config\":\"extensions:\\n  health_check:\\n    endpoint: \\\"0.0.0.0:13133\\\"\\n  pprof:\\n    endpoint: \\\"0.0.0.0:1777\\\"\\n  zpages:\\n    endpoint: \\\"0.0.0.0:55679\\\"\\n\\nreceivers:\\n  otlp:\\n    protocols:\\n      grpc:\\n        endpoint: \\\"0.0.0.0:4317\\\"\\n      http:\\n        endpoint: \\\"0.0.0.0:4318\\\"\\n  jaeger:\\n    protocols:\\n      grpc:\\n        endpoint: \\\"0.0.0.0:14250\\\"\\n      thrift_compact:\\n        endpoint: \\\"0.0.0.0:6831\\\"\\n      thrift_http:\\n        endpoint: \\\"0.0.0.0:14268\\\"\\n  loki:\\n    protocols:\\n      http:\\n        endpoint: \\\"0.0.0.0:4318\\\"\\n  prometheus/node:\\n    config:\\n      scrape_configs:\\n        - job_name: 'node-exporter'\\n          scrape_interval: 10s\\n          static_configs:\\n            - targets: ['prometheus-prometheus-node-exporter.monitoring.svc:9100']\\n              labels:\\n                job: 'node-exporter'\\n                cluster: 'clustername'\\n\\nprocessors:\\n  batch:\\n    timeout: 200ms\\n    send_batch_size: 1024\\n    send_batch_max_size: 4096\\n  memory_limiter:\\n    check_interval: 5s\\n    limit_percentage: 80\\n    spike_limit_mib: 64\\n    limit_mib: 512\\n  groupbytrace: {}\\n  resource:\\n    attributes:\\n      - key: cluster.name\\n        value: clustername\\n        action: insert\\n      - key: exporter\\n        value: \\\"node-exporter\\\"\\n        action: insert\\n  resourcedetection:\\n    detectors: [env, system]\\n    override: false\\n    timeout: 2s\\n  transform:\\n    log_statements:\\n      - context: log\\n        statements:\\n          - set(attributes[\\\"processing.note\\\"], \\\"Log enriched with resource and trace context\\\")\\n          - set(attributes[\\\"service.name\\\"], resource.attributes[\\\"service.name\\\"])\\n          - set(attributes[\\\"k8s.pod.name\\\"], resource.attributes[\\\"k8s.pod.name\\\"])\\n          - set(attributes[\\\"k8s.node.name\\\"], resource.attributes[\\\"k8s.node.name\\\"])\\n          - set(attributes[\\\"host.name\\\"], resource.attributes[\\\"host.name\\\"])\\n          - set(attributes[\\\"cluster.name\\\"], resource.attributes[\\\"cluster.name\\\"])\\n          - set(attributes[\\\"trace_id\\\"], trace_id)\\n          - set(attributes[\\\"span_id\\\"], span_id)\\n  metricstransform:\\n    transforms:\\n      - include: system.cpu.usage\\n        action: update\\n        operations:\\n          - action: add_label\\n            new_label: host.name\\n            new_value: host.name\\n          - action: add_label\\n            new_label: cluster.name\\n            new_value: clustername\\n      - include: system.memory.usage\\n        action: update\\n        operations:\\n          - action: add_label\\n            new_label: host.name\\n            new_value: host.name\\n          - action: add_label\\n            new_label: cluster.name\\n            new_value: clustername\\n\\nexporters:\\n  debug:\\n    verbosity: detailed\\n  otlphttp:\\n    endpoint: http://loki-write.monitoring.svc.cluster.local:3100/otlp\\n    tls:\\n      insecure: true\\n    headers:\\n      X-Scope-OrgID: tenant-1\\n  prometheus:\\n    endpoint: \\\"0.0.0.0:8889\\\"  \\n    namespace: monitoring\\n    const_labels:\\n      k8s_cluster: clustername\\n    send_timestamps: true\\n    enable_open_metrics: true\\n  otlp/jaeger:\\n    endpoint: jaeger-collector.monitoring.svc.cluster.local:4317\\n    tls:\\n      insecure: true\\n\\nservice:\\n  telemetry:\\n    metrics:\\n      level: detailed\\n  extensions: [health_check, pprof, zpages]\\n  pipelines:\\n    metrics:\\n      receivers: [otlp, prometheus/node]\\n      processors: [memory_limiter, resourcedetection, resource, metricstransform, batch]\\n      exporters: [debug, prometheus]\\n    traces:\\n      receivers: [otlp, jaeger]\\n      processors: [memory_limiter, resourcedetection, resource, groupbytrace, batch]\\n      exporters: [debug, otlp/jaeger]\\n    logs:\\n      receivers: [otlp, loki]\\n      processors: [memory_limiter, resourcedetection, resource, transform, batch]\\n      exporters: [debug, otlphttp]\\n\",\"env\":[{\"name\":\"K8S_NODE_NAME\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"spec.nodeName\"}}},{\"name\":\"K8S_POD_NAME\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.name\"}}},{\"name\":\"K8S_POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}},{\"name\":\"K8S_POD_IP\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"status.podIP\"}}}],\"image\":\"otel/opentelemetry-collector-contrib:0.128.0\",\"mode\":\"deployment\",\"ports\":[{\"name\":\"otlp-grpc\",\"port\":4317,\"protocol\":\"TCP\"},{\"name\":\"otlp-http\",\"port\":4318,\"protocol\":\"TCP\"},{\"name\":\"jaeger-grpc\",\"port\":14250,\"protocol\":\"TCP\"},{\"name\":\"jaeger-thrift-c\",\"port\":6831,\"protocol\":\"TCP\"},{\"name\":\"jaeger-thrift-h\",\"port\":14268,\"protocol\":\"TCP\"},{\"name\":\"metrics\",\"port\":8888,\"protocol\":\"TCP\"},{\"name\":\"prom-metrics\",\"port\":8889,\"protocol\":\"TCP\"},{\"name\":\"health\",\"port\":13133,\"protocol\":\"TCP\"}],\"replicas\":2,\"resources\":{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"1600Mi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"800Mi\"}},\"serviceAccount\":\"opentelemetry-collector\"}}\n"}},"spec":{"config":"extensions:\n  health_check:\n    endpoint: \"0.0.0.0:13133\"\n  pprof:\n    endpoint: \"0.0.0.0:1777\"\n  zpages:\n    endpoint: \"0.0.0.0:55679\"\n\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: \"0.0.0.0:4317\"\n      http:\n        endpoint: \"0.0.0.0:4318\"\n  jaeger:\n    protocols:\n      grpc:\n        endpoint: \"0.0.0.0:14250\"\n      thrift_compact:\n        endpoint: \"0.0.0.0:6831\"\n      thrift_http:\n        endpoint: \"0.0.0.0:14268\"\n  loki:\n    protocols:\n      http:\n        endpoint: \"0.0.0.0:4318\"\n  prometheus/node:\n    config:\n      scrape_configs:\n        - job_name: 'node-exporter'\n          scrape_interval: 10s\n          static_configs:\n            - targets: ['prometheus-prometheus-node-exporter.monitoring.svc:9100']\n              labels:\n                job: 'node-exporter'\n                cluster: 'clustername'\n\nprocessors:\n  batch:\n    timeout: 200ms\n    send_batch_size: 1024\n    send_batch_max_size: 4096\n  memory_limiter:\n    check_interval: 5s\n    limit_percentage: 80\n    spike_limit_mib: 64\n    limit_mib: 512\n  groupbytrace: {}\n  resource:\n    attributes:\n      - key: cluster.name\n        value: clustername\n        action: insert\n      - key: exporter\n        value: \"node-exporter\"\n        action: insert\n  resourcedetection:\n    detectors: [env, system]\n    override: false\n    timeout: 2s\n  transform:\n    log_statements:\n      - context: log\n        statements:\n          - set(attributes[\"processing.note\"], \"Log enriched with resource and trace context\")\n          - set(attributes[\"service.name\"], resource.attributes[\"service.name\"])\n          - set(attributes[\"k8s.pod.name\"], resource.attributes[\"k8s.pod.name\"])\n          - set(attributes[\"k8s.node.name\"], resource.attributes[\"k8s.node.name\"])\n          - set(attributes[\"host.name\"], resource.attributes[\"host.name\"])\n          - set(attributes[\"cluster.name\"], resource.attributes[\"cluster.name\"])\n          - set(attributes[\"trace_id\"], trace_id)\n          - set(attributes[\"span_id\"], span_id)\n  metricstransform:\n    transforms:\n      - include: system.cpu.usage\n        action: update\n        operations:\n          - action: add_label\n            new_label: host.name\n            new_value: host.name\n          - action: add_label\n            new_label: cluster.name\n            new_value: clustername\n      - include: system.memory.usage\n        action: update\n        operations:\n          - action: add_label\n            new_label: host.name\n            new_value: host.name\n          - action: add_label\n            new_label: cluster.name\n            new_value: clustername\n\nexporters:\n  debug:\n    verbosity: detailed\n  otlphttp:\n    endpoint: http://loki-write.monitoring.svc.cluster.local:3100/otlp\n    tls:\n      insecure: true\n    headers:\n      X-Scope-OrgID: tenant-1\n  prometheus:\n    endpoint: \"0.0.0.0:8889\"  \n    namespace: monitoring\n    const_labels:\n      k8s_cluster: clustername\n    send_timestamps: true\n    enable_open_metrics: true\n  otlp/jaeger:\n    endpoint: jaeger-collector.monitoring.svc.cluster.local:4317\n    tls:\n      insecure: true\n\nservice:\n  telemetry:\n    metrics:\n      level: detailed\n  extensions: [health_check, pprof, zpages]\n  pipelines:\n    metrics:\n      receivers: [otlp, prometheus/node]\n      processors: [memory_limiter, resourcedetection, resource, metricstransform, batch]\n      exporters: [debug, prometheus]\n    traces:\n      receivers: [otlp, jaeger]\n      processors: [memory_limiter, resourcedetection, resource, groupbytrace, batch]\n      exporters: [debug, otlp/jaeger]\n    logs:\n      receivers: [otlp, loki]\n      processors: [memory_limiter, resourcedetection, resource, transform, batch]\n      exporters: [debug, otlphttp]\n","ports":[{"name":"otlp-grpc","port":4317,"protocol":"TCP"},{"name":"otlp-http","port":4318,"protocol":"TCP"},{"name":"jaeger-grpc","port":14250,"protocol":"TCP"},{"name":"jaeger-thrift-c","port":6831,"protocol":"TCP"},{"name":"jaeger-thrift-h","port":14268,"protocol":"TCP"},{"name":"metrics","port":8888,"protocol":"TCP"},{"name":"prom-metrics","port":8889,"protocol":"TCP"},{"name":"health","port":13133,"protocol":"TCP"}],"resources":{"limits":{"cpu":"1000m"}}}}
to:
Resource: "opentelemetry.io/v1beta1, Resource=opentelemetrycollectors", GroupVersionKind: "opentelemetry.io/v1beta1, Kind=OpenTelemetryCollector"
Name: "otel-collector", Namespace: "monitoring"
for: "/azp/agent/_work/28/s/kustomize/overlays/EDIT/05opentelemetry/kustomization-output.yaml": error when patching "/azp/agent/_work/28/s/kustomize/overlays/EDIT/05opentelemetry/kustomization-output.yaml": admission webhook "mopentelemetrycollectorbeta.kb.io" denied the request: json: cannot unmarshal string into Go struct field OpenTelemetryCollectorSpec.spec.config of type v1beta1.Config
*** Done





#collector.yaml 

apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector
  namespace: monitoring
  labels:
    app: opentelemetry-collector
spec:
  mode: deployment
  image: otel/opentelemetry-collector-contrib:0.128.0
  serviceAccount: opentelemetry-collector
  replicas: 2
  
  ports:
    - name: otlp-grpc
      port: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      protocol: TCP
    - name: jaeger-grpc
      port: 14250
      protocol: TCP
    - name: jaeger-thrift-c  
      port: 6831
      protocol: TCP
    - name: jaeger-thrift-h  
      port: 14268
      protocol: TCP
    - name: metrics
      port: 8888
      protocol: TCP
    - name: prom-metrics
      port: 8889
      protocol: TCP
    - name: health
      port: 13133
      protocol: TCP

  env:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: K8S_POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: K8S_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP

  resources:
    requests:
      cpu: 500m
      memory: 800Mi
    limits:
      cpu: 1000m
      memory: 1600Mi

  config: |
    extensions:
      health_check:
        endpoint: "0.0.0.0:13133"
      pprof:
        endpoint: "0.0.0.0:1777"
      zpages:
        endpoint: "0.0.0.0:55679"

    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
      jaeger:
        protocols:
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_compact:
            endpoint: "0.0.0.0:6831"
          thrift_http:
            endpoint: "0.0.0.0:14268"
      loki:
        protocols:
          http:
            endpoint: "0.0.0.0:4318"
      prometheus/node:
        config:
          scrape_configs:
            - job_name: 'node-exporter'
              scrape_interval: 10s
              static_configs:
                - targets: ['prometheus-prometheus-node-exporter.monitoring.svc:9100']
                  labels:
                    job: 'node-exporter'
                    cluster: 'clustername'

    processors:
      batch:
        timeout: 200ms
        send_batch_size: 1024
        send_batch_max_size: 4096
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_mib: 64
        limit_mib: 512
      groupbytrace: {}
      resource:
        attributes:
          - key: cluster.name
            value: clustername
            action: insert
          - key: exporter
            value: "node-exporter"
            action: insert
      resourcedetection:
        detectors: [env, system]
        override: false
        timeout: 2s
      transform:
        log_statements:
          - context: log
            statements:
              - set(attributes["processing.note"], "Log enriched with resource and trace context")
              - set(attributes["service.name"], resource.attributes["service.name"])
              - set(attributes["k8s.pod.name"], resource.attributes["k8s.pod.name"])
              - set(attributes["k8s.node.name"], resource.attributes["k8s.node.name"])
              - set(attributes["host.name"], resource.attributes["host.name"])
              - set(attributes["cluster.name"], resource.attributes["cluster.name"])
              - set(attributes["trace_id"], trace_id)
              - set(attributes["span_id"], span_id)
      metricstransform:
        transforms:
          - include: system.cpu.usage
            action: update
            operations:
              - action: add_label
                new_label: host.name
                new_value: host.name
              - action: add_label
                new_label: cluster.name
                new_value: clustername
          - include: system.memory.usage
            action: update
            operations:
              - action: add_label
                new_label: host.name
                new_value: host.name
              - action: add_label
                new_label: cluster.name
                new_value: clustername

    exporters:
      debug:
        verbosity: detailed
      otlphttp:
        endpoint: http://loki-write.monitoring.svc.cluster.local:3100/otlp
        tls:
          insecure: true
        headers:
          X-Scope-OrgID: tenant-1
      prometheus:
        endpoint: "0.0.0.0:8889"  
        namespace: monitoring
        const_labels:
          k8s_cluster: clustername
        send_timestamps: true
        enable_open_metrics: true
      otlp/jaeger:
        endpoint: jaeger-collector.monitoring.svc.cluster.local:4317
        tls:
          insecure: true

    service:
      telemetry:
        metrics:
          level: detailed
      extensions: [health_check, pprof, zpages]
      pipelines:
        metrics:
          receivers: [otlp, prometheus/node]
          processors: [memory_limiter, resourcedetection, resource, metricstransform, batch]
          exporters: [debug, prometheus]
        traces:
          receivers: [otlp, jaeger]
          processors: [memory_limiter, resourcedetection, resource, groupbytrace, batch]
          exporters: [debug, otlp/jaeger]
        logs:
          receivers: [otlp, loki]
          processors: [memory_limiter, resourcedetection, resource, transform, batch]
          exporters: [debug, otlphttp]


#serviceMetric.yaml
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: monitoring
  labels:
    app: opentelemetry-collector
spec:
  selector:
    app: opentelemetry-collector
  ports:
    - name: prom-metrics
      port: 8889
      targetPort: 8889
    - name: metrics
      port: 8888
      targetPort: 8888
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
    - name: otlp-http
      port: 4318
      targetPort: 4318

#base/Kustomization 

namespace: monitoring

resources:
- collector.yaml
- serviceMetric.yaml






#overlay
#ServiceMonitor.yaml

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector-monitor
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: opentelemetry-collector
  namespaceSelector:
    matchNames: ["monitoring"]
  endpoints:
    - port: prom-metrics
      interval: 10s
      path: /metrics
      honorLabels: true
      relabelings:
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)


#collector-patch.yaml

apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector
  namespace: monitoring
spec:
  resources:
    limits:
      cpu: "1000m"
  ports:
    - name: otlp-grpc
      port: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      protocol: TCP
    - name: jaeger-grpc
      port: 14250
      protocol: TCP
    - name: jaeger-thrift-c
      port: 6831
      protocol: TCP
    - name: jaeger-thrift-h
      port: 14268
      protocol: TCP
    - name: metrics
      port: 8888
      protocol: TCP
    - name: prom-metrics
      port: 8889
      protocol: TCP
    - name: health
      port: 13133
      protocol: TCP


#Overlay/kustomization

namespace: monitoring

resources:
- ../../base

patches:
  - target:
      kind: MutatingWebhookConfiguration
      name: opentelemetry-operator-mutating-webhook-configuration
    path: opentelemetry-webhook-patch.yaml

  - target:
      kind: ServiceMonitor
      name: otel-collector-monitor
    path: ServiceMonitor.yaml

  - target:
      kind: OpenTelemetryCollector
      name: otel-collector
    path: collector-patch.yaml
