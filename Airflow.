apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: opentelemetry-collector
  namespace: monitoring
  labels:
    app: opentelemetry-collector
spec:
  mode: deployment
  image: otel/opentelemetry-collector-contrib:0.128.0
  serviceAccount: opentelemetry-collector
  replicas: 2

  resources:
    requests:
      cpu: 500m
      memory: 800Mi
    limits:
      cpu: 1000m
      memory: 1600Mi

  env:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: K8S_POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: K8S_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP

  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
      jaeger:
        protocols:
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_http:
            endpoint: "0.0.0.0:14268"
          thrift_compact:
            endpoint: "0.0.0.0:6831"
      loki:
        protocols:
          http:
            endpoint: "0.0.0.0:3500"
            include_metadata: true
          grpc:
            endpoint: "0.0.0.0:3600"
        use_incoming_timestamp: true

    processors:
      batch:
        timeout: 200ms
        send_batch_size: 1024
        send_batch_max_size: 4096
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_mib: 64
        limit_mib: 512
      resource:
        attributes:
          - key: service.name
            from_attribute: service.name
            action: upsert
          - key: k8s.container.name
            from_attribute: k8s.container.name
            action: insert
          - key: k8s.namespace.name
            from_attribute: k8s.namespace.name
            action: insert
          - key: k8s.pod.name
            from_attribute: k8s.pod.name
            action: insert
          - key: k8s.node.name
            from_attribute: k8s.node.name
            action: insert
          - key: host.name
            from_attribute: host.name
            action: insert
          - key: service.namespace
            from_attribute: service.namespace
            action: upsert
          - key: cluster.name
            value: "clustername"
            action: insert
      groupbytrace: {}
      transform:
        log_statements:
          - context: log
            statements:
              - set(attributes["processing.note"], "Log enriched with resource and trace context")
              - set(attributes["service.name"], resource.attributes["service.name"])
              - set(attributes["k8s.pod.name"], resource.attributes["k8s.pod.name"])
              - set(attributes["k8s.node.name"], resource.attributes["k8s.node.name"])
              - set(attributes["host.name"], resource.attributes["host.name"])
              - set(attributes["cluster.name"], resource.attributes["cluster.name"])
              - set(attributes["trace_id"], trace_id)
              - set(attributes["span_id"], span_id)
      metricstransform:
        transforms:
          - include: system.cpu.usage
            action: update
            operations:
              - action: add_label
                new_label: host.name
                new_value: host.name
              - action: add_label
                new_label: cluster.name
                new_value: clustername
          - include: system.memory.usage
            action: update
            operations:
              - action: add_label
                new_label: host.name
                new_value: host.name
              - action: add_label
                new_label: cluster.name
                new_value: clustername

    exporters:
      debug:
        verbosity: basic
      otlphttp:
        endpoint: "http://loki-write.monitoring.svc.cluster.local:3100/otlp"
        tls:
          insecure: true
      prometheus:
        endpoint: "0.0.0.0:9464"
        namespace: "monitoring"
        const_labels:
          k8s_cluster: "EDDVAKSCLUS01"
        send_timestamps: true
        enable_open_metrics: true
      otlp/jaeger:
        endpoint: "http://jaeger-collector.monitoring.svc.cluster.local:4317"
        tls:
          insecure: true
        headers:
          "X-Tenant-ID": "default"

    extensions:
      health_check:
        endpoint: "0.0.0.0:13133"
      pprof:
        endpoint: "0.0.0.0:1777"
      zpages:
        endpoint: "0.0.0.0:55679"

    service:
      telemetry:
        metrics:
          level: basic
        logs:
          level: debug
      extensions: [health_check, pprof, zpages]
      pipelines:
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, resource, metricstransform, batch]
          exporters: [debug, prometheus]
        traces:
          receivers: [otlp, jaeger]
          processors: [memory_limiter, resource, groupbytrace, batch]
          exporters: [debug, otlp/jaeger]
        logs:
          receivers: [otlp, loki]
          processors: [memory_limiter, resource, transform, batch]
          exporters: [debug, otlphttp]



We are currently blocked on the ingress controller issue. In the meantime, there’s a pending ticket requesting the AKS inventory for the DEV environment. I’d like to address that while we work on connecting all the pieces for the main issue. We’ve agreed to resume troubleshooting tomorrow."
















######################################################################################################################################################
#######################################################################################################################################################


processors:
  transform:
    log_statements:
      - context: log
        statements:
          - set(attributes["processing.note"], "Log enriched with resource and trace context")
          - set(attributes["service.name"], resource.attributes["service.name"])
          - set(attributes["k8s.pod.name"], resource.attributes["k8s.pod.name"])
          - set(attributes["host.name"], resource.attributes["host.name"])
          - set(attributes["trace_id"], trace_id)
          - set(attributes["span_id"], span_id)




 transform:
    log_statements:
      - context: log
        statements:
          - set(attributes["processing.note"], "Log enriched with span context")
          - set(attributes["service.name"], resource.attributes["service.name"])
          - set(attributes["k8s.pod"], resource.attributes["k8s.pod.name"])
          - merge_maps(attributes, resource.attributes, "insert")
          - merge_maps(attributes, span.attributes, "insert")

  # Metric transformations (optional)
  metricstransform:
    transforms:
      - include: system.cpu.usage
        action: update
        operations:
          - action: add_label
            new_label: host.name
            new_value: host.name
      - include: system.memory.usage
        action: update
        operations:
          - action: add_label
            new_label: host.name
            new_value: host.name

           

 












# grafana-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: your-namespace
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1  # Simple rewrite
    nginx.ingress.kubernetes.io/app-root: /grafana/  # Redirects /grafana to /grafana/
spec:
  rules:
  - http:
      paths:
      - path: /grafana(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: grafana-proxy  # Points to ExternalName service
            port:
              number: 3000



prometheus:
  additionalServiceMonitors:
    - name: otel-collector-monitor
      selector:
        matchLabels:
          app.kubernetes.io/name: opentelemetry-collector
      namespaceSelector:
        matchNames:
          - monitoring
      endpoints:
        - port: prometheus
          interval: 10s
          path: /metrics







# OpenTelemetry Collector Helm Values Configuration
# Deploy with: helm upgrade --install otel-collector open-telemetry/opentelemetry-collector -f values.yaml -n monitoring

mode: "deployment"
nameOverride: "opentelemetry-collector"
fullnameOverride: "opentelemetry-collector"
replicaCount: 2

configMap:
  create: true

image:
  repository: "otel/opentelemetry-collector-contrib"  # Using contrib for more processors
  pullPolicy: IfNotPresent
  tag: "0.96.0"  # Using a stable version

resources:
  requests:
    cpu: 500m
    memory: 800Mi
  limits:
    cpu: 1000m
    memory: 1600Mi

serviceAccount:
  create: true

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

  processors:
    batch:
      timeout: 200ms
      send_batch_size: 1024
      send_batch_max_size: 4096
    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_mib: 64
      limit_mib: 512

  exporters:
    debug:
      verbosity: basic
    prometheus:
      endpoint: "0.0.0.0:8889"
      namespace: "monitoring"
      const_labels:
        k8s_cluster: "EDDVAKSCLUS01"
      send_timestamps: true
      enable_open_metrics: true

  extensions:
    health_check:
      endpoint: 0.0.0.0:13133

  service:
    telemetry:
      logs:
        level: debug
    extensions: [health_check]
    pipelines:
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [debug, prometheus]

service:
  ports:
    otlp-grpc:
      enabled: true
      port: 4317
    otlp-http:
      enabled: true
      port: 4318
    prometheus:
      enabled: true
      port: 8889

livenessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 15
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 7
  targetCPUUtilizationPercentage: 90
  targetMemoryUtilizationPercentage: 90




########
update



