Since the Grafana instance in AKS relies on the UI rather than the API and is exposed via the NGINX Ingress Controller, Azure API Management (APIM) is not the optimal solution. Below are my recommended alternatives:

Private Internal Load Balancer + Azure Private Endpoint – Provides a highly secure, internal-only access model.

Azure Application Gateway with WAF in Internal Mode – Offers layer-7 routing, security (WAF), and private exposure.

Kubernetes-Native API Gateways – Lightweight, scalable, and better aligned with AKS-native traffic management.

controller:
  extraEnvs:
    - name: NGINX_DEBUG
      value: "1"


nginx.ingress.kubernetes.io/proxy-pass-headers: "Authorization"
nginx.ingress.kubernetes.io/auth-url: "https://your-apim-instance.azure-api.net/auth"
nginx.ingress.kubernetes.io/auth-signin: "https://your-apim-instance.azure-api.net/login"


kubectl get secret tls-good-abc123 -o jsonpath='{.data.tls\.crt}' | base64 --decode > secret.crt

 Grafana UI (web app) securely to users via APIM, enforce SSO via Azure AD, and route traffic to the internal NGINX Ingress Controller in AKS.






Since the Grafana instance deployed in AKS is primarily accessed via its user interface (UI) rather than a RESTful API, exposing it through Azure API Management (APIM) is not an ideal fit. Instead, the following alternatives are better aligned with secure, scalable, and internal-only access requirements:

Private Internal Load Balancer with Azure Private Endpoint
Leverages Azure networking to provide secure access exclusively from within trusted virtual networks, ensuring the Grafana UI remains completely inaccessible from the public internet.

Azure Application Gateway with Web Application Firewall (WAF) in Internal Mode
Enables advanced Layer 7 routing, TLS termination, and security controls through WAF, while restricting access to internal network traffic only. This is well-suited for enterprise-grade security postures.

Kubernetes-Native API Gateways (e.g., NGINX, Kong, Istio)
Offers a lightweight and cloud-native approach for managing internal traffic routing within AKS. These solutions are highly customizable and integrate seamlessly with Kubernetes networking, making them a practical option for securely exposing Grafana and other services.























APIM → NGINX Ingress IP (ILB) → Ingress Rule → ClusterIP service → Pod
User → APIM (external) → Internal Load Balancer (ILB) Ingress Controller → NGINX Ingress Controller → Kubernetes Service (ClusterIP) → Pod

Request Completed" method=GET path=/grafana/login status=302 remote_addr=127.0.0.1 time_ms=3 duration=3.413573ms size=18657 referer= handler=notfound status_source=server
https://us04web.zoom.us/j/78985446518?pwd=65M1mLDyJcwejNbULMWUJ3aZJtioiD.1

The application has two Kubernetes services: a ClusterIP for internal traffic and a LoadBalancer to allow Azure services like APIM to access the ingress controller. However, the microservice is currently mapped only to the ClusterIP, which suggests that its external exposure via APIM is happening indirectly. The identified gap is a missing or unclear routing configuration between the Ingress controller and APIM, as the service itself is not directly exposed externally

There is an ongoing session regarding this issue, but it remains unresolved. We are still awaiting an update since configuring the AKS ingress, as we are currently unable to access the Grafana URL as expected


If APIM calls the LoadBalancer IP on port 443, the traffic is forwarded by the ingress-nginx-controller-internal service to the NGINX ingress controller pod's https port.




<!--
    - Policies are applied in the order they appear.
    - Position <base/> inside a section to inherit policies from the outer scope.
    - Comments within policies are not preserved.
-->
<!-- Add policies as children to the <inbound>, <outbound>, <backend>, and <on-error> elements -->
<policies>
    <!-- Throttle, authorize, validate, cache, or transform the requests -->
    <inbound>
        <base />
    </inbound>
    <!-- Control if and how the requests are forwarded to services  -->
    <backend>
        <base />
    </backend>
    <!-- Customize the responses -->
    <outbound>
        <base />
    </outbound>
    <!-- Handle exceptions and customize error responses  -->
    <on-error>
        <base />
    </on-error>
</policies>










apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: eddv3-hbt
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/proxy-http-version: "1.1"
    # Replaced configuration-snippet with standard annotations
    nginx.ingress.kubernetes.io/proxy-set-headers: "eddv3-hbt/grafana-proxy-headers"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  ingressClassName: nginx
  rules:
  - host: eddv3-hbt
    http:
      paths:
      - path: /grafana(/|$)(.*)  # Fixed path pattern
        pathType: Prefix
        backend:
          service:
            name: grafana-proxy
            port:
              number: 80


apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-proxy-headers
  namespace: eddv3-hbt
data:
  Host: "$host"
  X-Real-IP: "$remote_addr"
  X-Forwarded-Proto: "$scheme"
  X-Forwarded-For: "$proxy_add_x_forwarded_for"
