apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector
  namespace: monitoring
  labels:
    release: kube-prometheus-stack  # ⚠️ Must match Prometheus release label
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: otel-collector
  namespaceSelector:
    matchNames:
      - monitoring
  endpoints:
    - port: prometheus
      path: /metrics
      interval: 30s





7d13h



mode: "deployment"
nameOverride: "opentelemetry-collector"
fullnameOverride: "opentelemetry-collector"
replicaCount: 2

image:
  repository: "otel/opentelemetry-collector-contrib"
  pullPolicy: IfNotPresent
  tag: "0.128.0"

resources:
  requests:
    cpu: 500m
    memory: 800Mi
  limits:
    cpu: 1000m
    memory: 1600Mi

serviceAccount:
  create: true

configMap:
  create: true
  existingName: ""

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
    jaeger:
      protocols:
        grpc:
          endpoint: 0.0.0.0:14250
        thrift_http:
          endpoint: 0.0.0.0:14268
        thrift_compact:
          endpoint: 0.0.0.0:6831
    loki:
      protocols:
        http:
          endpoint: 0.0.0.0:3500
          include_metadata: true 
        grpc:
          endpoint: 0.0.0.0:3600
      use_incoming_timestamp: true

  processors:
    batch: 
      timeout: 200ms
      send_batch_size: 1024 
      send_batch_max_size: 4096

    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_mib: 64
      limit_mib: 512

    resourcedetection:
      detectors: [env, system, kubernetes]
      timeout: 2s
      override: false

    resource:
      attributes:
        - key: service.name
          from_attribute: service.name
          action: insert
        - key: k8s.container.name
          from_attribute: k8s.container.name
          action: insert
        - key: k8s.namespace.name
          from_attribute: k8s.namespace.name
          action: insert
        - key: k8s.pod.name
          from_attribute: k8s.pod.name
          action: insert
        - key: host.name
          from_attribute: host.name
          action: insert
        - key: service.name
          from_attribute: service.name
          action: upsert 
        - key: service.namespace
          from_attribute: service.namespace
          action: upsert

    groupbytrace: {}

    transform:
      log_statements:
        - context: log
          statements:
            - set(attributes["processing.note"], "Log enriched with resource and trace context")
            - set(attributes["service.name"], resource.attributes["service.name"])
            - set(attributes["k8s.pod.name"], resource.attributes["k8s.pod.name"])
            - set(attributes["host.name"], resource.attributes["host.name"])
            - set(attributes["trace_id"], trace_id)
            - set(attributes["span_id"], span_id)

    metricstransform:
      transforms:
        - include: system.cpu.usage
          action: update
          operations:
            - action: add_label
              new_label: host.name
              new_value: host.name
        - include: system.memory.usage
          action: update
          operations:
            - action: add_label
              new_label: host.name
              new_value: host.name

  exporters:
    debug: 
      verbosity: basic
    otlphttp:
      endpoint: "http://loki.monitoring.svc.cluster.local:3100/otlp"
      tls:
        insecure: true
    prometheus:
      endpoint: "0.0.0.0:8889"
      namespace: "monitoring"
      const_labels:
        k8s_cluster: "EDDVAKSCLUS01"
      send_timestamps: true 
      enable_open_metrics: true 
    otlp:
      endpoint: "http://jaeger-collector.monitoring.svc.cluster.local:4317"
      tls:
        insecure: true
    otlp/jaeger:
      endpoint: "http://jaeger-collector.monitoring.svc.cluster.local:4317"
      tls:
        insecure: true

  extensions:
    health_check:
      endpoint: 0.0.0.0:13133

  service:
    telemetry:
      logs: 
        level: debug
    extensions:
      - health_check
    pipelines: 
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, resourcedetection, resource, metricstransform, batch]
        exporters: [debug, prometheus]

      traces:
        receivers: [otlp, jaeger]
        processors: [memory_limiter, resourcedetection, resource, groupbytrace, batch]
        exporters: [debug, otlp/jaeger]

      logs:
        receivers: [otlp, loki]
        processors: [resourcedetection, resource, transform, memory_limiter, batch]
        exporters: [debug, otlphttp]

ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
  jaeger-compact:
    enabled: true
    containerPort: 6831
    protocol: UDP
  jaeger-thrift:
    enabled: true
    containerPort: 14268
  jaeger-grpc:
    enabled: true
    containerPort: 14250
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    hostPort: 8888
    protocol: TCP 
  prometheus:
    enabled: true
    containerPort: 8889
    servicePort: 8889
    hostPort: 8889
    protocol: TCP 

livenessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 15
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 7
  targetCPUUtilizationPercentage: 90
  targetMemoryUtilizationPercentage: 90

useGOMEMLIMIT: true















######################################################################################################################################################
#######################################################################################################################################################


processors:
  transform:
    log_statements:
      - context: log
        statements:
          - set(attributes["processing.note"], "Log enriched with resource and trace context")
          - set(attributes["service.name"], resource.attributes["service.name"])
          - set(attributes["k8s.pod.name"], resource.attributes["k8s.pod.name"])
          - set(attributes["host.name"], resource.attributes["host.name"])
          - set(attributes["trace_id"], trace_id)
          - set(attributes["span_id"], span_id)




 transform:
    log_statements:
      - context: log
        statements:
          - set(attributes["processing.note"], "Log enriched with span context")
          - set(attributes["service.name"], resource.attributes["service.name"])
          - set(attributes["k8s.pod"], resource.attributes["k8s.pod.name"])
          - merge_maps(attributes, resource.attributes, "insert")
          - merge_maps(attributes, span.attributes, "insert")

  # Metric transformations (optional)
  metricstransform:
    transforms:
      - include: system.cpu.usage
        action: update
        operations:
          - action: add_label
            new_label: host.name
            new_value: host.name
      - include: system.memory.usage
        action: update
        operations:
          - action: add_label
            new_label: host.name
            new_value: host.name

           

 












# grafana-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: your-namespace
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1  # Simple rewrite
    nginx.ingress.kubernetes.io/app-root: /grafana/  # Redirects /grafana to /grafana/
spec:
  rules:
  - http:
      paths:
      - path: /grafana(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: grafana-proxy  # Points to ExternalName service
            port:
              number: 3000



prometheus:
  additionalServiceMonitors:
    - name: otel-collector-monitor
      selector:
        matchLabels:
          app.kubernetes.io/name: opentelemetry-collector
      namespaceSelector:
        matchNames:
          - monitoring
      endpoints:
        - port: prometheus
          interval: 10s
          path: /metrics







# OpenTelemetry Collector Helm Values Configuration
# Deploy with: helm upgrade --install otel-collector open-telemetry/opentelemetry-collector -f values.yaml -n monitoring

mode: "deployment"
nameOverride: "opentelemetry-collector"
fullnameOverride: "opentelemetry-collector"
replicaCount: 2

configMap:
  create: true

image:
  repository: "otel/opentelemetry-collector-contrib"  # Using contrib for more processors
  pullPolicy: IfNotPresent
  tag: "0.96.0"  # Using a stable version

resources:
  requests:
    cpu: 500m
    memory: 800Mi
  limits:
    cpu: 1000m
    memory: 1600Mi

serviceAccount:
  create: true

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

  processors:
    batch:
      timeout: 200ms
      send_batch_size: 1024
      send_batch_max_size: 4096
    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_mib: 64
      limit_mib: 512

  exporters:
    debug:
      verbosity: basic
    prometheus:
      endpoint: "0.0.0.0:8889"
      namespace: "monitoring"
      const_labels:
        k8s_cluster: "EDDVAKSCLUS01"
      send_timestamps: true
      enable_open_metrics: true

  extensions:
    health_check:
      endpoint: 0.0.0.0:13133

  service:
    telemetry:
      logs:
        level: debug
    extensions: [health_check]
    pipelines:
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [debug, prometheus]

service:
  ports:
    otlp-grpc:
      enabled: true
      port: 4317
    otlp-http:
      enabled: true
      port: 4318
    prometheus:
      enabled: true
      port: 8889

livenessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 15
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 7
  targetCPUUtilizationPercentage: 90
  targetMemoryUtilizationPercentage: 90




########
update




# OpenTelemetry Collector Helm Values Configuration
# Deploy with: helm install otel-collector open-telemetry/opentelemetry-collector -f values.yaml

mode: "deployment"
nameOverride: "opentelemetry-collector"
fullnameOverride: "opentelemetry-collector"
replicaCount: 2


configMap:
  create: true
  existingName: ""

internalTelemetryViaOTLP:
  endpoint: ""
  headers: []
  traces:
    enabled: false
  metrics:
    enabled: false
  logs:
    enabled: false

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
    jaeger:
      protocols:
        grpc:
          endpoint: 0.0.0.0:14250
        thrift_http:
          endpoint: 0.0.0.0:14268
        thrift_compact:
          endpoint: 0.0.0.0:6831
    # kubeletstats:
    #   insecure_skip_verify: true
    # prometheus:
    #   config:
    #     scrape_configs:
    #       - job_name: opentelemetry-collector
    #         scrape_interval: 10s
    #         static_configs:
    #           - targets:
    #             - 0.0.0.0:8888 

    prometheus:
      config:
        scrape_configs:
          - job_name: "otentelemetry-collector"
            scrape_interval: 10s
            static_configs:
              - targets: ["opentelemetry-collector:8888"]
            relabel_configs:
              - source_labels: [__address__]
                target_label: __tmp_hash
                modulus: 1
                action: hashmod
              - source_labels: [__tmp_hash]
                regex: "0"
                action: keep

  processors:
    batch: 
      timeout: 200ms
      send_batch_size: 1024 
      send_batch_max_size: 4096
    # resourcedetection:
    #   detectors: [env, system, azure]
    #   timeout: 2s 
    #   override: false
    # transform:
    #   error_mode: ignore
    #   log_statements:
    #     - context: log
    #       statements: 
    #         - set(resource.attributes["service.name"], attributes["service_name"])
    #         - delete_key(attributes, "service_name")
    # Uncomment and configure memory limiter if needed
    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_mib: 64
      limit_mib: 512

  exporters:
    debug: 
      verbosity: basic
    otlphttp:
      endpoint: "http://loki-write.monitoring.svc.cluster.local:3100/otlp"
      tls:
        insecure: true
    prometheus:
      endpoint: "0.0.0.0:8889"
      namespace: "monitoring"
      const_labels:
        k8s_cluster: "EDDVAKSCLUS01"
      send_timestamps: true 
      enable_open_metrics: true 
    otlp:
      endpoint: "http://jaeger-collector.monitoring.svc.cluster.local:4317"
      tls:
        insecure: true
    # Named OTLP exporter for Jaeger
    otlp/jaeger:
      endpoint: "http://jaeger-collector.monitoring.svc.cluster.local:4317"
      tls:
        insecure: true
  
  extensions:
    health_check:
      endpoint: 0.0.0.0:13133

  service:
    telemetry:
      logs: 
        level: debug
    extensions:
      - health_check
    pipelines: 
      metrics:
        receivers: [otlp, prometheus]
        processors: [memory_limiter, batch]
        exporters: [debug, prometheus]
      traces:
        receivers: [otlp, jaeger]
        processors: [memory_limiter, batch]
        exporters: [debug, otlp/jaeger]
      logs:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [debug, otlphttp]

image:
  repository: "otel/opentelemetry-collector"
  pullPolicy: IfNotPresent
  tag: "0.123.0"

resources:
  requests:
    cpu: 500m
    memory: 800Mi
  limits:
    cpu: 1000m
    memory: 1600Mi

serviceAccount:
  create: true

ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
  jaeger-compact:
    enabled: true
    containerPort: 6831
    protocol: UDP
  jaeger-thrift:
    enabled: true
    containerPort: 14268
  jaeger-grpc:
    enabled: true
    containerPort: 14250
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    hostPort: 8888
    protocol: TCP 
  prometheus:
    enabled: true
    containerPort: 8889
    servicePort: 8889
    hostPort: 8889
    protocol: TCP 

extraEnvs:
  - name: POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP

livenessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 15
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 7
  targetCPUUtilizationPercentage: 90
  targetMemoryUtilizationPercentage: 90

useGOMEMLIMIT: true

---
prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    additionalScrapeConfigs:
      - job_name: 'opentelemetry-collector'
        scrape_interval: 10s
        static_configs:
          - targets: ["opentelemetry-collector.monitoring.svc.cluster.local:8889"]

---

prometheus:
  additionalServiceMonitors:
    - name: otel-collector-monitor
      selector:
        matchLabels:
          app.kubernetes.io/name: opentelemetry-collector
      namespaceSelector:
        matchNames:
          - monitoring
      endpoints:
        - port: prometheus
          interval: 10s







########################################################################################################


prometheus:
  additionalServiceMonitors:
    - name: otel-collector-monitor
      selector:
        matchLabels:
          app.kubernetes.io/name: opentelemetry-collector
      namespaceSelector:
        matchNames:
          - monitoring
      endpoints:
        - port: prometheus
          interval: 10s




prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    additionalScrapeConfigs:
      - job_name: 'otel-collector'
        scrape_interval: 10s
        static_configs:
          - targets: "opentelemetry-collector:8889"







prometheus:
      endpoint: "0.0.0.0:9090"
      namespace: "monitoring"
      const_labels:
        k8s_cluster: "EDDVAKSCLUS01"
      send_timestamps: true 
      enable_open_metrics: true 

    debug   prometheusexporter@v0.123.0/accumulator.go:226  Accumulate histogram.....
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_processor_batch_metadata_cardinality
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_processor_batch_timeout_trigger_send_total
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_processor_incoming_items_total
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_processor_outgoing_items_total
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_receiver_accepted_metric_points_total
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_receiver_refused_metric_points_total
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: promhttp_metric_handler_errors_total
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: up
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: scrape_duration_seconds
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: scrape_samples_scraped
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: scrape_samples_post_metric_relabeling
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: scrape_series_added
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_exporter_queue_size
2025-07-09T20:47:07.138Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_exporter_send_failed_metric_points_total
2025-07-09T20:47:07.139Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_exporter_sent_metric_points_total
2025-07-09T20:47:07.139Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_exporter_queue_capacity
2025-07-09T20:47:07.139Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_process_memory_rss_bytes
2025-07-09T20:47:07.139Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_process_runtime_heap_alloc_bytes
2025-07-09T20:47:07.139Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_process_runtime_total_alloc_bytes_total
2025-07-09T20:47:07.139Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_process_runtime_total_sys_memory_bytes
2025-07-09T20:47:07.139Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_process_uptime_seconds_total
2025-07-09T20:47:07.139Z        debug   prometheusexporter@v0.123.0/accumulator.go:79   accumulating metric: otelcol_process_cpu_seconds_total
2025-07-09T20:47:07.139Z        info    Metrics {"resource metrics": 1, "metrics": 24, "data points": 29}
2025-07-09T20:47:08.237Z        debug   memorylimiter@v0.123.0/memorylimiter.go:178     Currently used memory.  {"cur_mem_mib": 11}
2025-07-09T20:47:13.236Z        debug   memorylimiter@v0.123.0/memorylimiter.go:178     Currently used memory.  {"cur_mem_mib": 11}



Error: cannot start pipelines: failed to start "prometheus" exporter: listen tcp 10.0.0.26:9090: bind: cannot assign requested address; failed to shutdown pipelines: no existing monitoring routine is running; no existing monitoring routine is running; no existing monitoring routine is running


 kubectl get svc -n monitoring | findstr opentelemetry-collector
opentelemetry-collector                          ClusterIP   10.0.0.29    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,8889/TCP,9411/TCP   25h

kubectl get svc -n monitoring | findstr prometheus
kube-prometheus-stack-alertmanager               ClusterIP   10.0.0.78    <none>        9093/TCP,8080/TCP                                                           46h
kube-prometheus-stack-kube-state-metrics         ClusterIP   10.0.0.120   <none>        8080/TCP                                                                    46h
kube-prometheus-stack-operator                   ClusterIP   10.0.0.112   <none>        443/TCP                                                                     46h
kube-prometheus-stack-prometheus                 ClusterIP   10.0.0.26    <none>        9090/TCP,8080/TCP                                                           46h
kube-prometheus-stack-prometheus-node-exporter   ClusterIP   10.0.0.36    <none>        9100/TCP                                                                    46h
prometheus-operated                              ClusterIP   None         <none>        9090/TCP                                                                    46h
# OpenTelemetry Collector Helm Values Configuration
# Deploy with: helm install otel-collector open-telemetry/opentelemetry-collector -f values.yaml

mode: "deployment"
nameOverride: "opentelemetry-collector"
fullnameOverride: "opentelemetry-collector"
replicaCount: 2

configMap:
  create: true
  existingName: ""

internalTelemetryViaOTLP:
  endpoint: ""
  headers: []
  traces:
    enabled: false
  metrics:
    enabled: false
  logs:
    enabled: false

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
    jaeger:
      protocols:
        grpc:
          endpoint: 0.0.0.0:14250
        thrift_http:
          endpoint: 0.0.0.0:14268
        thrift_compact:
          endpoint: 0.0.0.0:6831
    # kubeletstats:
    #   insecure_skip_verify: true
    # prometheus:
    #   config:
    #     scrape_configs:
    #       - job_name: opentelemetry-collector
    #         scrape_interval: 10s
    #         static_configs:
    #           - targets:
    #             - 0.0.0.0:8888 

    prometheus:
      config:
        scrape_configs:
          - job_name: "otentelemetry-collector"
            scrape_interval: 10s
            static_configs:
              - targets: ["opentelemetry-collector:8888"]
            relabel_configs:
              - source_labels: [__address__]
                target_label: __tmp_hash
                modulus: 1
                action: hashmod
              - source_labels: [__tmp_hash]
                regex: "0"
                action: keep

  processors:
    batch: 
      timeout: 200ms
      send_batch_size: 1024 
      send_batch_max_size: 4096
    # transform:
    #   error_mode: ignore
    #   log_statements:
    #     - context: log
    #       statements: 
    #         - set(resource.attributes["service.name"], attributes["service_name"])
    #         - delete_key(attributes, "service_name")
    # Uncomment and configure memory limiter if needed
    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_mib: 64
      limit_mib: 512

  exporters:
    debug: 
      verbosity: basic
    otlphttp:
      endpoint: "http://loki-write.monitoring.svc.cluster.local:3100/otlp"
      tls:
        insecure: true
    prometheus:
      endpoint: "0.0.0.0:9090"
    otlp:
      endpoint: "http://jaeger-collector.monitoring.svc.cluster.local:4317"
      tls:
        insecure: true
    # Named OTLP exporter for Jaeger
    otlp/jaeger:
      endpoint: "http://jaeger-collector.monitoring.svc.cluster.local:4317"
      tls:
        insecure: true
  
  extensions:
    health_check:
      endpoint: 0.0.0.0:13133

  service:
    telemetry:
      logs: 
        level: debug
    extensions:
      - health_check
    pipelines: 
      metrics:
        receivers: [otlp, prometheus]
        processors: [memory_limiter, batch]
        exporters: [debug, prometheus]
      traces:
        receivers: [otlp, jaeger]
        processors: [batch, memory_limiter]
        exporters: [debug, otlp/jaeger]
      logs:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [debug, otlphttp]

image:
  repository: "otel/opentelemetry-collector"
  pullPolicy: IfNotPresent
  tag: "0.123.0"

resources:
  requests:
    cpu: 500m
    memory: 800Mi
  limits:
    cpu: 1000m
    memory: 1600Mi

serviceAccount:
  create: true

ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
  jaeger-compact:
    enabled: true
    containerPort: 6831
    protocol: UDP
  jaeger-thrift:
    enabled: true
    containerPort: 14268
  jaeger-grpc:
    enabled: true
    containerPort: 14250
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    hostPort: 8888
    protocol: TCP 
  prometheus:
    enabled: true
    containerPort: 8889
    servicePort: 8889
    hostPort: 8889
    protocol: TCP 

extraEnvs:
  - name: POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP

livenessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 15
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 7
  targetCPUUtilizationPercentage: 90
  targetMemoryUtilizationPercentage: 90

useGOMEMLIMIT: true














#####################################################################################

Impact: Pipeline delays/failures due to DNS resolution timeouts.

Environment: [Specify if this occurs in Kubernetes, Docker, shell executor, etc.]

Troubleshooting steps taken so far:

Verified DNS server availability and response times.

Adjusted concurrent job limits in config.toml (if applicable).

[Add any other relevant steps attempted.]

Could you provide guidance on resolving this issue? Possible considerations:



- path: /grafana(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: grafana-service
            port:
              number: 3000

- path: /grafana(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: grafana-service
            port:
              number: 3000

kubectl get ingress --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace}/{.metadata.name}{"\n"}{range .spec.rules[*]}{"  Host: "}{.host}{"\n"}{range .http.paths[*]}{"    "}{.path} → {.backend.service.name}:{.backend.service.port.number}{"\n"}{end}{end}{end}'
for file in *; do [ -f "$file" ] && echo "=== $file ===" && cat "$file"; done

Integrating Loki with OTEL Collector and Grafana frequently encounters obstacles due to sparse documentation and complex configuration requirements. Based on my experience with active deployments and tested implementation approaches, the most prevalent integration challenges observed include:
Primary Deployment Issues:

Incorrectly configured Loki data source settings in Grafana
Defective OTEL Collector pipeline configurations


This battle-tested approach eliminates guesswork and provides reliable patterns for successful Loki integration across diverse environments.




The lack of comprehensive documentation for Loki can indeed make integration frustrating. Based on extensive real-world experience, here's a battle-tested approach to make Loki work with OTEL Collector and Grafana.
Misconfigured Loki data source in Grafana
Incorrect OTEL Collector pipeline configuration






otel-collector
=============
2025-06-26T17:04:13.187Z	debug	otlphttpexporter@v0.123.0/otlp.go:177	Preparing to make HTTP request	{"url": "http://loki-gateway.eddv3-hbt.svc.cluster.local:3100/otlp/v1/metrics"}
2025-06-26T17:04:13.188Z	info	internal/retry_sender.go:133	Exporting failed. Will retry the request after interval.	{"error": "failed to make an HTTP request: Post \"http://loki-gateway.eddv3-hbt.svc.cluster.local:3100/otlp/v1/metrics\": context deadline exceeded", "interval": "4.869718034s"}
2025-06-26T17:04:16.107Z	info	internal/retry_sender.go:133	Exporting failed. Will retry the request after interval.	{"error": "failed to make an HTTP reque

Jaeger
======
Data source connected, but no services received. Verify that Jaeger is configured properly.  

https://stackoverflow.com/questions/78362832/exporting-opentelemetry-data-in-node-js-to-otlp-endpoint-and-to-local-console


https://www.npmjs.com/package/%40opentelemetry/exporter-logs-otlp-http


https://www.npmjs.com/package/%40opentelemetry/exporter-logs-otlp-grpc


https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/


https://us04web.zoom.us/j/76978410627?pwd=D5vQ10Mhkb06CbYJJULeNJyD3ORaWb.1










---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  labels:
    app: opentelemetry
    component: otel-collector
spec:
  ports:
  - name: metrics # Default endpoint for querying metrics.
    port: 8888
  selector:
    component: otel-collector


---
####


POD_NAME=$(kubectl get pods -n cicd -l app=host-access -o jsonpath='{.items[0].metadata.name}')

additionalChartEnvValues: |
  - name: APP_NAME
    value: "flex-demo"
  - name: LOG_LEVEL
    value: "debug"
  - name: OTEL_EXPORTER_OTLP_ENDPOINT
    value: "http://$(HOST_IP):4317"
  - name: HOST_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: OTEL_LOG_LEVEL
    value: "debug"

additionalChartEnvValues: |
    { "name": "APP_NAME", "value": "flex-demo" },
    { "name": "LOG_LEVEL", "value": "debug" },
    { "name": "OTEL_EXPORTER_OTLP_ENDPOINT", "value": "http://$(HOST_IP):4317" },
    { "name": "HOST_IP", "valueFrom": { "fieldRef": { "fieldPath": status.hostIP } } },
    { "name": "OTEL_LOG_LEVEL", "value": "debug" }





#####










https://opentelemetry.io/docs/platforms/kubernetes/getting-started/


receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

processors:
  batch:
  transform:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          - set(resource.attributes["service.name"], attributes["service.name"])
          - delete_key(attributes, "service.name")

exporters:
  debug:
    verbosity: detailed
  otlp/jaeger:
    endpoint: jaeger:4317
    tls:
      insecure: true
  otlphttp:
    endpoint: http://loki:3100/otlp
    tls:
      insecure: true
  prometheus:
    endpoint: 0.0.0.0:9464

extensions:
  health_check:
    endpoint: 0.0.0.0:13133

service:
  extensions: [health_check]
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp/jaeger, debug]
    logs:
      receivers: [otlp]
      processors: [batch, transform]
      exporters: [otlphttp, debug]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [prometheus, debug]
  telemetry:
    logs:
      level: debug
 










"http://$(OTEL_COLLECTOR_SERVICE_HOST):4317"

/azp/agent/_work/_tool/helm/3.18.3/x64/linux-amd64/helm upgrade --namespace good9009 --install --values /azp/agent/_work/35/s/devops/helm-charts/grafana-monitor/otel-values-dev.yaml --wait --timeout 5m0s --create-namespace my-opentelemetry-collector open-telemetry/opentelemetry-collector
Error: UPGRADE FAILED: values don't meet the specifications of the schema(s) in the following chart(s):
opentelemetry-collector:
- (root): Additional property rbac is not allowed
- (root): Additional property env is not allowed

##[error]Error: UPGRADE FAILED: values don't meet the specifications of the schema(s) in the following chart(s):
opentelemetry-collector:
- (root): Additional property rbac is not allowed
- (root): Additional property env is not allowed






helm install my-opentelemetry-collector open-telemetry/opentelemetry-collector --set mode=<value> --set image.repository="ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-k8s" --set command.name="otelcol-k8s"




/azp/agent/_work/_tool/helm/3.18.3/x64/linux-amd64/helm upgrade --namespace eddv3-hbt --install --values /azp/agent/_work/35/s/devops/helm-charts/grafana-monitor/otel-values-dev.yaml --wait --timeout 5m0s --create-namespace my-opentelemetry-collector open-telemetry/opentelemetry-collector
Error: UPGRADE FAILED: deployments.apps "my-opentelemetry-collector" not found
##[error]Error: UPGRADE FAILED: deployments.apps "my-opentelemetry-collector" not found



POD_NAME=$(kubectl get pods -n cicd -l app=host-access -o jsonpath='{.items[0].metadata.name}')

additionalChartEnvValues: |
  - name: APP_NAME
    value: "flex-demo"
  - name: LOG_LEVEL
    value: "debug"
  - name: OTEL_EXPORTER_OTLP_ENDPOINT
    value: "http://$(HOST_IP):4317"
  - name: HOST_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: OTEL_LOG_LEVEL
    value: "debug"

additionalChartEnvValues: |
    { "name": "APP_NAME", "value": "flex-demo" },
    { "name": "LOG_LEVEL", "value": "debug" },
    { "name": "OTEL_EXPORTER_OTLP_ENDPOINT", "value": "http://$(HOST_IP):4317" },
    { "name": "HOST_IP", "valueFrom": { "fieldRef": { "fieldPath": status.hostIP } } },
    { "name": "OTEL_LOG_LEVEL", "value": "debug" }

env:
  - name: OTEL_EXPORTER_OTLP_ENDPOINT
    value: "http://$(HOST_IP):4318"
  - name: HOST_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP




env:
- name: OTEL_EXPORTER_OTLP_ENDPOINT
  value: "http://$(HOST_IP):4318"
- name: HOST_IP
  valueFrom:
    fieldRef:
      fieldPath: status.hostIP


For monitoring AKS (Azure Kubernetes Service) resources with Prometheus, you'll want to use several exporters depending on what specific data you need to capture:

## Core Kubernetes Metrics
**kube-state-metrics** - This is the primary exporter for Kubernetes object state metrics. It provides information about:
- Pod status, resource requests/limits
- Deployment, ReplicaSet, Service status
- Node conditions and capacity
- Persistent Volume claims
- ConfigMaps, Secrets, and other Kubernetes objects

## Node-Level Metrics
**node-exporter** - Captures host-level metrics from each node:
- CPU, memory, disk, network utilization
- Filesystem usage and availability
- Hardware and OS metrics

## Container Metrics
**cAdvisor** - Built into kubelet, provides container-level metrics:
- Container CPU and memory usage
- Network and filesystem I/O
- Container lifecycle events

## Azure-Specific Metrics
**azure-metrics-exporter** - For Azure-specific resources and services:
- Azure Monitor metrics
- AKS-specific Azure resource metrics
- Integration with Azure services

## Application Metrics
**Custom application exporters** - Language-specific exporters for your applications:
- Prometheus client libraries for Go, Java, Python, .NET, etc.
- Application-specific metrics and business KPIs

## Installation Approach
The most common setup uses the **Prometheus Operator** or **kube-prometheus-stack** Helm chart, which bundles:
- Prometheus server
- kube-state-metrics
- node-exporter
- Grafana for visualization
- AlertManager for notifications

For AKS specifically, you might also consider **Azure Monitor for containers**, which can complement Prometheus by providing deeper integration with Azure services while still allowing you to export metrics to Prometheus.

Which specific aspects of your AKS resources are you most interested in monitoring?





You could add or update a Geo node database record, setting the name to match this machine's Geo node name 

you could set this machine's Geo node name to match the name of an existing database record: 

https://www.bing.com/search?pglt=2083&q=OpenTelemetry+Operator+AKS&cvid=222e1e2d1a024e788327f7908f5ecb46&gs_lcrp=EgRlZGdlKgYIABBFGDkyBggAEEUYOTIGCAEQABhAMggIAhDpBxj8VdIBCDMwNDlqMGoxqAIAsAIA&FORM=ANNAB1&PC=U531

https://www.yuribacciarini.com/secrets-store-csi-driver-vs-external-secrets-in-a-nutshel/

https://learn.microsoft.com/en-us/azure/aks/aksarc/secrets-store-csi-driver

https://github.com/kubernetes-sigs/secrets-store-csi-driver/blob/main/test/bats/tests/azure/azure_v1_secretproviderclass_ns.yaml

https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/sync-with-k8s-secrets/


 gitlab-backup create STRATEGY=copy BACKUP=$(date "+%Y-%m-%d_%H.%M.%S")



FROM alpine:3.19 AS certs
RUN apk --update add ca-certificates

FROM golang:1.23.6 AS build-stage
WORKDIR /usr/bin/otelcol

COPY collector/manifest.yaml manifest.yaml
COPY collector/config.yaml config.yaml
COPY protos/ protos
COPY processors/ processors
COPY extensions/ extensions
RUN --mount=type=cache,target=/root/.cache/go-build GO111MODULE=on go install go.opentelemetry.io/collector/cmd/builder@v0.120.0
RUN --mount=type=cache,target=/root/.cache/go-build builder --config manifest.yaml

RUN --mount=type=cache,target=/root/.cache/go-build ./_build/col validate --config config.yaml

FROM gcr.io/distroless/base:latest

ARG USER_UID=10001
USER ${USER_UID}

COPY --from=build-stage /usr/bin/otelcol/config.yaml /etc/otelcol/config.yaml
COPY --from=certs /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt
COPY --chmod=755 --from=build-stage /usr/bin/otelcol/_build/col /otelcol

ENTRYPOINT ["/otelcol"]
CMD ["--config", "/etc/otelcol/config.yaml"]
# `4137` and `4318`: OTLP
# `12001`: remotettap
EXPOSE 4317 4318 12001






Due to current environmental constraints, the recommended solution cannot be implemented at this time. The existing CSI driver has a known limitation, as the SecretSync functionality does not appear to be enabled. Enabling this feature may require significant time and effort.

As a result, we will proceed with an alternative solution. A risk will be formally documented to highlight this environmental limitation and its implications.






https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/sync-with-k8s-secrets/
https://github.com/kubernetes-sigs/secrets-store-csi-driver/blob/main/test/bats/tests/azure/azure_v1_secretproviderclass_ns.yaml
https://stackoverflow.com/questions/70154936/secret-is-not-creating-in-aks-after-fetching-it-with-csi-driver
https://github.com/kubernetes-sigs/secrets-store-csi-driver/blob/main/test/bats/tests/vault/pod-vault-inline-volume-secretproviderclass.yaml
https://learn.microsoft.com/en-us/azure/aks/aksarc/secrets-store-csi-driver







During the integration of the Azure Key Vault Provider for Secrets Store CSI Driver, I deployed a SecretProviderClass resource using the azure provider to fetch secrets from Key Vault. These secrets are intended to be used as environment variables for the Grafana Helm chart deployment.

As part of the process, I confirmed that the secrets are successfully pulled and mounted into a pod, which indicates that the SecretProviderClass is functional. However, the expected Kubernetes Secret resource is not being created, even though the secretObjects section is defined in the secrestore.yaml file.

After reviewing the documentation (Azure CSI Provider: Sync with K8s Secrets), I believe the current Helm setup might be missing the following critical configuration:

secrets-store-csi-driver.syncSecret.enabled=true

This setting is required to enable the secret sync controller and to ensure that the Helm release provisions the necessary ClusterRole and ClusterRoleBinding for managing and syncing secrets from Azure Key Vault to native Kubernetes Secrets.

Using only volume-mounted secrets is not suitable in this case because:
* Grafana is deployed via Helm and configured using environment variables.
* The secrets need to be available as native Kubernetes secrets to be referenced by the Helm chart as envFrom.secretRef, rather than being mounted into the pod’s filesystem.

Action Request:
Please review and update the Helm release values to explicitly enable secret syncing by setting:

syncSecret:
  enabled: true

This should resolve the issue and ensure secrets are properly synced and consumable by Grafana as environment variables.

Thanks,















We’re excited to share that we’ve successfully deployed the External Secrets Operator (ESO) in our environment, leveraging Federated Identity Credentials (FIC) backed by Microsoft Entra ID for authentication.

Key Benefits:
✅ Eliminates long-lived secrets: No more manual rotation or exposure risks of static credentials.
✅ Secure by design: Uses short-lived, auto-rotated tokens via Microsoft’s identity platform.
✅ Compliance-friendly: Aligns with zero-trust principles and audit requirements (e.g., SOC 2, ISO 27001).

This approach simplifies secret management while enhancing security. We’d appreciate your formal consent to adopt this as our standard for secret management.

Next Steps:
Review: Documentation/architecture is available [link to Confluence/SharePoint].

Feedback: Share concerns or questions by [date].

Consent: Reply to this thread with your approval or schedule a discussion.

Thanks for your collaboration!









RUN apk update && apk add --no-cache \
bash \
gcc \
musl-dev \
postgresql-dev \
libffi-dev \
openssl-dev \
build-base \
python3-dev \
py3-pip \
abseil-cpp-dev \
re2
   

# Install Python packages
RUN pip install --no-cache-dir \
psycopg2-binary \
pytz \
pybind11 \
apache-airflow

