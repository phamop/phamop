Integrating Loki with OTEL Collector and Grafana frequently encounters obstacles due to sparse documentation and complex configuration requirements. Based on my experience with active deployments and tested implementation approaches, the most prevalent integration challenges observed include:
Primary Deployment Issues:

Incorrectly configured Loki data source settings in Grafana
Defective OTEL Collector pipeline configurations


This battle-tested approach eliminates guesswork and provides reliable patterns for successful Loki integration across diverse environments.




The lack of comprehensive documentation for Loki can indeed make integration frustrating. Based on extensive real-world experience, here's a battle-tested approach to make Loki work with OTEL Collector and Grafana.
Misconfigured Loki data source in Grafana
Incorrect OTEL Collector pipeline configuration






otel-collector
=============
2025-06-26T17:04:13.187Z	debug	otlphttpexporter@v0.123.0/otlp.go:177	Preparing to make HTTP request	{"url": "http://loki-gateway.eddv3-hbt.svc.cluster.local:3100/otlp/v1/metrics"}
2025-06-26T17:04:13.188Z	info	internal/retry_sender.go:133	Exporting failed. Will retry the request after interval.	{"error": "failed to make an HTTP request: Post \"http://loki-gateway.eddv3-hbt.svc.cluster.local:3100/otlp/v1/metrics\": context deadline exceeded", "interval": "4.869718034s"}
2025-06-26T17:04:16.107Z	info	internal/retry_sender.go:133	Exporting failed. Will retry the request after interval.	{"error": "failed to make an HTTP reque

Jaeger
======
Data source connected, but no services received. Verify that Jaeger is configured properly.  

https://stackoverflow.com/questions/78362832/exporting-opentelemetry-data-in-node-js-to-otlp-endpoint-and-to-local-console


https://www.npmjs.com/package/%40opentelemetry/exporter-logs-otlp-http


https://www.npmjs.com/package/%40opentelemetry/exporter-logs-otlp-grpc


https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/


https://us04web.zoom.us/j/76978410627?pwd=D5vQ10Mhkb06CbYJJULeNJyD3ORaWb.1










---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  labels:
    app: opentelemetry
    component: otel-collector
spec:
  ports:
  - name: metrics # Default endpoint for querying metrics.
    port: 8888
  selector:
    component: otel-collector


---
####


POD_NAME=$(kubectl get pods -n cicd -l app=host-access -o jsonpath='{.items[0].metadata.name}')

additionalChartEnvValues: |
  - name: APP_NAME
    value: "flex-demo"
  - name: LOG_LEVEL
    value: "debug"
  - name: OTEL_EXPORTER_OTLP_ENDPOINT
    value: "http://$(HOST_IP):4317"
  - name: HOST_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: OTEL_LOG_LEVEL
    value: "debug"

additionalChartEnvValues: |
    { "name": "APP_NAME", "value": "flex-demo" },
    { "name": "LOG_LEVEL", "value": "debug" },
    { "name": "OTEL_EXPORTER_OTLP_ENDPOINT", "value": "http://$(HOST_IP):4317" },
    { "name": "HOST_IP", "valueFrom": { "fieldRef": { "fieldPath": status.hostIP } } },
    { "name": "OTEL_LOG_LEVEL", "value": "debug" }





#####










https://opentelemetry.io/docs/platforms/kubernetes/getting-started/


receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

processors:
  batch:
  transform:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          - set(resource.attributes["service.name"], attributes["service.name"])
          - delete_key(attributes, "service.name")

exporters:
  debug:
    verbosity: detailed
  otlp/jaeger:
    endpoint: jaeger:4317
    tls:
      insecure: true
  otlphttp:
    endpoint: http://loki:3100/otlp
    tls:
      insecure: true
  prometheus:
    endpoint: 0.0.0.0:9464

extensions:
  health_check:
    endpoint: 0.0.0.0:13133

service:
  extensions: [health_check]
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp/jaeger, debug]
    logs:
      receivers: [otlp]
      processors: [batch, transform]
      exporters: [otlphttp, debug]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [prometheus, debug]
  telemetry:
    logs:
      level: debug
 










"http://$(OTEL_COLLECTOR_SERVICE_HOST):4317"

/azp/agent/_work/_tool/helm/3.18.3/x64/linux-amd64/helm upgrade --namespace good9009 --install --values /azp/agent/_work/35/s/devops/helm-charts/grafana-monitor/otel-values-dev.yaml --wait --timeout 5m0s --create-namespace my-opentelemetry-collector open-telemetry/opentelemetry-collector
Error: UPGRADE FAILED: values don't meet the specifications of the schema(s) in the following chart(s):
opentelemetry-collector:
- (root): Additional property rbac is not allowed
- (root): Additional property env is not allowed

##[error]Error: UPGRADE FAILED: values don't meet the specifications of the schema(s) in the following chart(s):
opentelemetry-collector:
- (root): Additional property rbac is not allowed
- (root): Additional property env is not allowed






helm install my-opentelemetry-collector open-telemetry/opentelemetry-collector --set mode=<value> --set image.repository="ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-k8s" --set command.name="otelcol-k8s"




/azp/agent/_work/_tool/helm/3.18.3/x64/linux-amd64/helm upgrade --namespace eddv3-hbt --install --values /azp/agent/_work/35/s/devops/helm-charts/grafana-monitor/otel-values-dev.yaml --wait --timeout 5m0s --create-namespace my-opentelemetry-collector open-telemetry/opentelemetry-collector
Error: UPGRADE FAILED: deployments.apps "my-opentelemetry-collector" not found
##[error]Error: UPGRADE FAILED: deployments.apps "my-opentelemetry-collector" not found



POD_NAME=$(kubectl get pods -n cicd -l app=host-access -o jsonpath='{.items[0].metadata.name}')

additionalChartEnvValues: |
  - name: APP_NAME
    value: "flex-demo"
  - name: LOG_LEVEL
    value: "debug"
  - name: OTEL_EXPORTER_OTLP_ENDPOINT
    value: "http://$(HOST_IP):4317"
  - name: HOST_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: OTEL_LOG_LEVEL
    value: "debug"

additionalChartEnvValues: |
    { "name": "APP_NAME", "value": "flex-demo" },
    { "name": "LOG_LEVEL", "value": "debug" },
    { "name": "OTEL_EXPORTER_OTLP_ENDPOINT", "value": "http://$(HOST_IP):4317" },
    { "name": "HOST_IP", "valueFrom": { "fieldRef": { "fieldPath": status.hostIP } } },
    { "name": "OTEL_LOG_LEVEL", "value": "debug" }

env:
  - name: OTEL_EXPORTER_OTLP_ENDPOINT
    value: "http://$(HOST_IP):4318"
  - name: HOST_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP




env:
- name: OTEL_EXPORTER_OTLP_ENDPOINT
  value: "http://$(HOST_IP):4318"
- name: HOST_IP
  valueFrom:
    fieldRef:
      fieldPath: status.hostIP


For monitoring AKS (Azure Kubernetes Service) resources with Prometheus, you'll want to use several exporters depending on what specific data you need to capture:

## Core Kubernetes Metrics
**kube-state-metrics** - This is the primary exporter for Kubernetes object state metrics. It provides information about:
- Pod status, resource requests/limits
- Deployment, ReplicaSet, Service status
- Node conditions and capacity
- Persistent Volume claims
- ConfigMaps, Secrets, and other Kubernetes objects

## Node-Level Metrics
**node-exporter** - Captures host-level metrics from each node:
- CPU, memory, disk, network utilization
- Filesystem usage and availability
- Hardware and OS metrics

## Container Metrics
**cAdvisor** - Built into kubelet, provides container-level metrics:
- Container CPU and memory usage
- Network and filesystem I/O
- Container lifecycle events

## Azure-Specific Metrics
**azure-metrics-exporter** - For Azure-specific resources and services:
- Azure Monitor metrics
- AKS-specific Azure resource metrics
- Integration with Azure services

## Application Metrics
**Custom application exporters** - Language-specific exporters for your applications:
- Prometheus client libraries for Go, Java, Python, .NET, etc.
- Application-specific metrics and business KPIs

## Installation Approach
The most common setup uses the **Prometheus Operator** or **kube-prometheus-stack** Helm chart, which bundles:
- Prometheus server
- kube-state-metrics
- node-exporter
- Grafana for visualization
- AlertManager for notifications

For AKS specifically, you might also consider **Azure Monitor for containers**, which can complement Prometheus by providing deeper integration with Azure services while still allowing you to export metrics to Prometheus.

Which specific aspects of your AKS resources are you most interested in monitoring?





You could add or update a Geo node database record, setting the name to match this machine's Geo node name 

you could set this machine's Geo node name to match the name of an existing database record: 

https://www.bing.com/search?pglt=2083&q=OpenTelemetry+Operator+AKS&cvid=222e1e2d1a024e788327f7908f5ecb46&gs_lcrp=EgRlZGdlKgYIABBFGDkyBggAEEUYOTIGCAEQABhAMggIAhDpBxj8VdIBCDMwNDlqMGoxqAIAsAIA&FORM=ANNAB1&PC=U531

https://www.yuribacciarini.com/secrets-store-csi-driver-vs-external-secrets-in-a-nutshel/

https://learn.microsoft.com/en-us/azure/aks/aksarc/secrets-store-csi-driver

https://github.com/kubernetes-sigs/secrets-store-csi-driver/blob/main/test/bats/tests/azure/azure_v1_secretproviderclass_ns.yaml

https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/sync-with-k8s-secrets/


 gitlab-backup create STRATEGY=copy BACKUP=$(date "+%Y-%m-%d_%H.%M.%S")



FROM alpine:3.19 AS certs
RUN apk --update add ca-certificates

FROM golang:1.23.6 AS build-stage
WORKDIR /usr/bin/otelcol

COPY collector/manifest.yaml manifest.yaml
COPY collector/config.yaml config.yaml
COPY protos/ protos
COPY processors/ processors
COPY extensions/ extensions
RUN --mount=type=cache,target=/root/.cache/go-build GO111MODULE=on go install go.opentelemetry.io/collector/cmd/builder@v0.120.0
RUN --mount=type=cache,target=/root/.cache/go-build builder --config manifest.yaml

RUN --mount=type=cache,target=/root/.cache/go-build ./_build/col validate --config config.yaml

FROM gcr.io/distroless/base:latest

ARG USER_UID=10001
USER ${USER_UID}

COPY --from=build-stage /usr/bin/otelcol/config.yaml /etc/otelcol/config.yaml
COPY --from=certs /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt
COPY --chmod=755 --from=build-stage /usr/bin/otelcol/_build/col /otelcol

ENTRYPOINT ["/otelcol"]
CMD ["--config", "/etc/otelcol/config.yaml"]
# `4137` and `4318`: OTLP
# `12001`: remotettap
EXPOSE 4317 4318 12001






Due to current environmental constraints, the recommended solution cannot be implemented at this time. The existing CSI driver has a known limitation, as the SecretSync functionality does not appear to be enabled. Enabling this feature may require significant time and effort.

As a result, we will proceed with an alternative solution. A risk will be formally documented to highlight this environmental limitation and its implications.






https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/sync-with-k8s-secrets/
https://github.com/kubernetes-sigs/secrets-store-csi-driver/blob/main/test/bats/tests/azure/azure_v1_secretproviderclass_ns.yaml
https://stackoverflow.com/questions/70154936/secret-is-not-creating-in-aks-after-fetching-it-with-csi-driver
https://github.com/kubernetes-sigs/secrets-store-csi-driver/blob/main/test/bats/tests/vault/pod-vault-inline-volume-secretproviderclass.yaml
https://learn.microsoft.com/en-us/azure/aks/aksarc/secrets-store-csi-driver







During the integration of the Azure Key Vault Provider for Secrets Store CSI Driver, I deployed a SecretProviderClass resource using the azure provider to fetch secrets from Key Vault. These secrets are intended to be used as environment variables for the Grafana Helm chart deployment.

As part of the process, I confirmed that the secrets are successfully pulled and mounted into a pod, which indicates that the SecretProviderClass is functional. However, the expected Kubernetes Secret resource is not being created, even though the secretObjects section is defined in the secrestore.yaml file.

After reviewing the documentation (Azure CSI Provider: Sync with K8s Secrets), I believe the current Helm setup might be missing the following critical configuration:

secrets-store-csi-driver.syncSecret.enabled=true

This setting is required to enable the secret sync controller and to ensure that the Helm release provisions the necessary ClusterRole and ClusterRoleBinding for managing and syncing secrets from Azure Key Vault to native Kubernetes Secrets.

Using only volume-mounted secrets is not suitable in this case because:
* Grafana is deployed via Helm and configured using environment variables.
* The secrets need to be available as native Kubernetes secrets to be referenced by the Helm chart as envFrom.secretRef, rather than being mounted into the pod’s filesystem.

Action Request:
Please review and update the Helm release values to explicitly enable secret syncing by setting:

syncSecret:
  enabled: true

This should resolve the issue and ensure secrets are properly synced and consumable by Grafana as environment variables.

Thanks,















We’re excited to share that we’ve successfully deployed the External Secrets Operator (ESO) in our environment, leveraging Federated Identity Credentials (FIC) backed by Microsoft Entra ID for authentication.

Key Benefits:
✅ Eliminates long-lived secrets: No more manual rotation or exposure risks of static credentials.
✅ Secure by design: Uses short-lived, auto-rotated tokens via Microsoft’s identity platform.
✅ Compliance-friendly: Aligns with zero-trust principles and audit requirements (e.g., SOC 2, ISO 27001).

This approach simplifies secret management while enhancing security. We’d appreciate your formal consent to adopt this as our standard for secret management.

Next Steps:
Review: Documentation/architecture is available [link to Confluence/SharePoint].

Feedback: Share concerns or questions by [date].

Consent: Reply to this thread with your approval or schedule a discussion.

Thanks for your collaboration!









RUN apk update && apk add --no-cache \
bash \
gcc \
musl-dev \
postgresql-dev \
libffi-dev \
openssl-dev \
build-base \
python3-dev \
py3-pip \
abseil-cpp-dev \
re2
   

# Install Python packages
RUN pip install --no-cache-dir \
psycopg2-binary \
pytz \
pybind11 \
apache-airflow

