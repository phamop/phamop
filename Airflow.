global:
  imageRegistry: null
  imagePullSecrets: []

rbac:
  create: true

serviceAccount:
  create: false

  labels: {}

  automountServiceAccountToken: true

replicas: 1

headlessService: false

automountServiceAccountToken: true

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  targetCPU: "60"
  targetMemory: ""
  behavior: {}

podDisruptionBudget: {}

deploymentStrategy:
  type: RollingUpdate

readinessProbe:
  httpGet:
    path: /api/health
    port: 3000

livenessProbe:
  httpGet:
    path: /api/health
    port: 3000
  initialDelaySeconds: 60
  timeoutSeconds: 30
  failureThreshold: 10


image:
  registry: docker.io
  repository: grafana/grafana
  tag: ""
  sha: ""
  pullPolicy: IfNotPresent

  pullSecrets: []

testFramework:
  enabled: true
  image:
    registry: docker.io
    repository: bats/bats
    tag: "v1.4.1"
  imagePullPolicy: IfNotPresent
  securityContext: {}
  containerSecurityContext: {}
  resources: {}

dnsPolicy: ~
dnsConfig: {}

securityContext:
  runAsNonRoot: true
  runAsUser: 472
  runAsGroup: 472
  fsGroup: 472

containerSecurityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  seccompProfile:
    type: RuntimeDefault

createConfigmap: true


extraConfigmapMounts: []


extraEmptyDirMounts: []

extraLabels: {}


downloadDashboardsImage:
  registry: docker.io
  repository: curlimages/curl
  tag: 8.9.1
  sha: ""
  pullPolicy: IfNotPresent

downloadDashboards:
  env: {}
  envFromSecret: ""
  resources: {}
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    seccompProfile:
      type: RuntimeDefault
  envValueFrom: {}


podPortName: grafana
gossipPortName: gossip

service:
  enabled: true
  type: ClusterIP
  ipFamilyPolicy: ""
  ipFamilies: []
  loadBalancerIP: ""
  loadBalancerClass: ""
  loadBalancerSourceRanges: []
  port: 80
  targetPort: 3000
  annotations: {}
  labels: {}
  portName: service
  appProtocol: ""
  sessionAffinity: ""

serviceMonitor:
  
  enabled: false
  path: /metrics
  labels: {}
  interval: 30s
  scheme: http
  tlsConfig: {}
  scrapeTimeout: 30s
  relabelings: []
  metricRelabelings: []
  basicAuth: {}
  targetLabels: []

extraExposePorts: []

hostAliases: []

ingress:
  enabled: true
  annotations: {}
  labels: {}
  path: /grafana

  pathType: Prefix

  hosts:
    - grafana
  extraPaths: []


  tls: []

route:
  main:
    enabled: false

    apiVersion: gateway.networking.k8s.io/v1
    kind: HTTPRoute

    annotations: {}
    labels: {}

    hostnames: []
    parentRefs: []

    matches:
      - path:
          type: PathPrefix
          value: /

    filters: []

    additionalRules: []

resources: {}

nodeSelector: {}

tolerations: []

affinity: {}

topologySpreadConstraints: []

extraInitContainers: []

extraContainers: ""

extraContainerVolumes: []

persistence:
  type: pvc
  enabled: true
  accessModes:
    - ReadWriteOnce
  size: 10Gi
  finalizers:
    - kubernetes.io/pvc-protection
  extraPvcLabels: {}
  disableWarning: false

  inMemory:
    enabled: false

  lookupVolumeName: true

initChownData:
  enabled: true




  image:
    registry: docker.io
    repository: library/busybox
    tag: "1.31.1"
    sha: ""
    pullPolicy: IfNotPresent

  resources: {}
  securityContext:
    runAsNonRoot: false
    runAsUser: 0
    seccompProfile:
      type: RuntimeDefault
    capabilities:
      add:
        - CHOWN

adminUser: admin

admin:
  existingSecret: ""
  userKey: admin-user
  passwordKey: admin-password




env: {}


envValueFrom: {}

envFromSecret: "azure-monitors"


envRenderSecret: {}

envFromSecrets: []

envFromConfigMaps: []

enableServiceLinks: true

extraSecretMounts: []
  
 
extraVolumeMounts: []

extraVolumes: []

lifecycleHooks: {}

plugins:
    - grafana-azure-monitor-datasource
  
datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Azure Monitor
      type: grafana-azure-monitor-datasource
      access: proxy
      jsonData:
        azureAuthType: clientsecret
        tenantId: $TENANTID
        clientId: $CLIENTID
        logAnalyticsDefaultWorkspace: "/subscriptions/5918db9c-25c1-4564-9079-665362a0b0c2/resourceGroups/loganalytics-general-cd/providers/Microsoft.OperationalInsights/workspaces/cd-general-01"
      secureJsonData:
        clientSecret: $CLIENTSECRET
      version: 1
    - name: loki 
      type: loki
      access: proxy
      url: http://loki-read.monitoring.svc.cluster.local:3100
      isDefault: false
      jsonData:
        maxLines: 1000
    - name: Jaeger
      type: jaeger
      access: proxy
      url: http://jaeger-query.monitoring.svc.cluster.local:16686
      isDefault: false
      jsonData:
        traceDuration: 24h
        traceURL: "/jaeger"
      secureJsonData: {}
    - name: Prometheus
      type: prometheus
      access: proxy
      isDefault: true
      jsonData:
        timeInterval: "15s"
    - name: Alertmanager
      type: Alertmanager
      access: proxy
      isDefault: false
      jsonData:
        timeInterval: "15s"


alerting: {}

notifiers: {}

dashboardProviders:
  dashboardproviders.yaml:
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: 'AKS Monitoring'
      type: file
      disableDeletion: false
      editable: true
      options:
        path: /var/lib/grafana/dashboards/default

dashboards: {}
dashboards:
  default:
    aks-node-pod-health-dashboard:
      json: |
        {
          "id": null,
          "uid": "akshealthdashboard",
          "title": "AKS Node & Pod Health",
          "tags": ["aks", "health"],
          "timezone": "browser",
          "schemaVersion": 36,
          "version": 1,
          "refresh": "30s",
          "panels": [
            {
              "type": "stat",
              "title": "Ready Nodes",
              "datasource": "Azure Monitor",
              "targets": [
                {
                  "query": "KubeNodeInventory | summarize count()",
                  "format": "time_series"
                }
              ],
              "gridPos": { "h": 4, "w": 6, "x": 0, "y": 0 }
            },
            {
              "type": "stat",
              "title": "Running Pods",
              "datasource": "Azure Monitor",
              "targets": [
                {
                  "query": "KubePodInventory | where Phase == 'Running' | summarize count()",
                  "format": "time_series"
                }
              ],
              "gridPos": { "h": 4, "w": 6, "x": 6, "y": 0 }
            },
            {
              "type": "stat",
              "title": "CrashLoopBackOff Pods",
              "datasource": "Azure Monitor",
              "targets": [
                {
                  "query": "KubePodInventory | where Reason == 'CrashLoopBackOff' | summarize count()",
                  "format": "time_series"
                }
              ],
              "gridPos": { "h": 4, "w": 6, "x": 12, "y": 0 }
            }
          ]
        }


alertingRules:
  AKSAlerts:
    groups:
      - name: AKS Alerts
        interval: 1m
        rules:
          - uid: pod-cpu-alert
            title: Pod CPU High
            condition: B
            data:
              - refId: A
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: <your-datasource-uid>
                model:
                  expr: |
                    InsightsMetrics
                    | where Namespace == "container.azm.ms"
                    | where Name == "cpuUsageNanoCores"
                    | summarize avg(Total) by bin(TimeGenerated, 1m)
                  format: time_series
                  intervalMs: 60000
                  maxDataPoints: 43200
              - refId: B
                type: classic_condition
                evaluator:
                  type: gt
                  params:
                    - 0.8
                operator:
                  type: and
                reducer:
                  type: avg
                query:
                  refId: A
                noDataState: OK
                execErrState: Error

notifiers:
  notifiers.yaml:
    notifiers:
      - name: Default Email
        type: email
        uid: default-email
        org_id: 1
        is_default: true
        settings:
          addresses: phamop@gmail.com
dashboardsConfigMaps: {}

grafana.ini:
  paths:
    data: /var/lib/grafana/
    logs: /var/log/grafana
    plugins: /var/lib/grafana/plugins
    provisioning: /etc/grafana/provisioning
  analytics:
    check_for_updates: true
  log:
    mode: console
  grafana_net:
    url: https://grafana.net
  server:
    domain: "{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ tpl (.Values.ingress.hosts | first) . }}{{ else }}''{{ end }}"
  server:
    root_url: https://eddvaksclus01-ingress.cmhc-schl.gc.ca/grafana
    serve_from_sub_path = true

ldap:
  enabled: false
  existingSecret: ""
  config: ""


shareProcessNamespace: false

smtp:
  existingSecret: ""
  userKey: "user"
  passwordKey: "password"

sidecar:
  image:
    registry: quay.io
    repository: kiwigrid/k8s-sidecar
    tag: 1.30.0
    sha: ""
  imagePullPolicy: IfNotPresent
  resources: {}
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    seccompProfile:
      type: RuntimeDefault
  enableUniqueFilenames: false
  readinessProbe: {}
  livenessProbe: {}
  alerts:
    enabled: false
    env: {}
    label: grafana_alert
    labelValue: ""
    searchNamespace: null
    watchMethod: WATCH
    resource: both
    reloadURL: "http://localhost:3000/api/admin/provisioning/alerting/reload"
    script: null
    skipReload: false
    initAlerts: false
    extraMounts: []
    sizeLimit: {}
  dashboards:
    enabled: false
    env: {}
    envValueFrom: {}
    SCProvider: true
    label: grafana_dashboard
    labelValue: ""
    folder: /tmp/dashboards
    defaultFolderName: null
    searchNamespace: null
    watchMethod: WATCH
    resource: both
    folderAnnotation: null
    reloadURL: "http://localhost:3000/api/admin/provisioning/dashboards/reload"
    script: null
    skipReload: false
    provider:
      name: sidecarProvider
      orgid: 1
      folder: ''
      folderUid: ''
      type: file
      disableDelete: false
      allowUiUpdates: false
      foldersFromFilesStructure: false
    extraMounts: []
    sizeLimit: {}
  datasources:
    enabled: false
    env: {}
    envValueFrom: {}
    label: grafana_datasource
    labelValue: ""
    searchNamespace: null
    watchMethod: WATCH
    resource: both
    reloadURL: "http://localhost:3000/api/admin/provisioning/datasources/reload"
    script: null
    skipReload: false
    initDatasources: false
    extraMounts: []
    sizeLimit: {}
  plugins:
    enabled: false
    env: {}
    label: grafana_plugin
    labelValue: ""
    searchNamespace: null
    watchMethod: WATCH
    resource: both
    reloadURL: "http://localhost:3000/api/admin/provisioning/plugins/reload"
    script: null
    skipReload: false
    initPlugins: true
    extraMounts: []
    sizeLimit: {}
  notifiers:
    enabled: false
    env: {}
    label: grafana_notifier
    labelValue: ""
    searchNamespace: null
    watchMethod: WATCH
    resource: both
    reloadURL: "http://localhost:3000/api/admin/provisioning/notifications/reload"
    script: null
    skipReload: false
    initNotifiers: false
    extraMounts: []
    sizeLimit: {}

namespaceOverride: ""

revisionHistoryLimit: 10

imageRenderer:
  deploymentStrategy: {}
  enabled: false
  replicas: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    targetCPU: "60"
    targetMemory: ""
    behavior: {}
  serverURL: ""
  renderingCallbackURL: ""
  image:
    registry: docker.io
    repository: grafana/grafana-image-renderer
    tag: latest
    sha: ""
    pullPolicy: Always
  env:
    HTTP_HOST: "0.0.0.0"
    XDG_CONFIG_HOME: /tmp/.chromium
    XDG_CACHE_HOME: /tmp/.chromium

  envValueFrom: {}

  serviceAccountName: ""
  automountServiceAccountToken: false
  securityContext: {}
  containerSecurityContext:
    seccompProfile:
      type: RuntimeDefault
    capabilities:
      drop: ['ALL']
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
  podAnnotations: {}
  hostAliases: []
  priorityClassName: ''
  service:
    enabled: true
    portName: 'http'
    port: 8081
    targetPort: 8081
    appProtocol: ""
  serviceMonitor:
    enabled: false
    path: /metrics
    labels: {}
    interval: 1m
    scheme: http
    tlsConfig: {}
    scrapeTimeout: 30s
    relabelings: []
    targetLabels: []
  grafanaProtocol: http
  grafanaSubPath: ""
  podPortName: http
  revisionHistoryLimit: 10
  networkPolicy:
    limitIngress: true
    limitEgress: false
    extraIngressSelectors: []
  resources: {}
  nodeSelector: {}

  tolerations: []

  affinity: {}


  extraConfigmapMounts: []

  extraSecretMounts: []

  extraVolumeMounts: []

  extraVolumes: []

networkPolicy:
  enabled: false
  ingress: true
  allowExternal: true
  explicitNamespacesSelector: {}
  egress:
    enabled: false
    blockDNSResolution: false
    ports: []
    to: []

enableKubeBackwardCompatibility: false
useStatefulSet: false

extraObjects: []

assertNoLeakedSecrets: true









Error: failed to get config: cannot unmarshal the configuration: decoding failed due to the following error(s):

'service.telemetry' has invalid keys: metrics_address
2025/07/16 22:41:57 collector server run finished with error: failed to get config: cannot unmarshal the configuration: decoding failed due to the following error(s):

'service.telemetry' has invalid keys: metrics_address




apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otelcollector
  namespace: monitoring
  labels:
    app: opentelemetry-collector
spec:
  mode: deployment
  image: otel/opentelemetry-collector-contrib:0.128.0
  serviceAccount: opentelemetry-collector
  replicas: 2

  env:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: K8S_POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: K8S_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP

  resources:
    requests:
      cpu: 500m
      memory: 800Mi
    limits:
      cpu: 1000m
      memory: 1600Mi

  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
      jaeger:
        protocols:
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_http:
            endpoint: "0.0.0.0:14268"
          thrift_compact:
            endpoint: "0.0.0.0:6831"
      loki:
        protocols:
          http:
            endpoint: "0.0.0.0:3500"
            include_metadata: true
          grpc:
            endpoint: "0.0.0.0:3600"
        use_incoming_timestamp: true

    processors:
      batch:
        timeout: 200ms
        send_batch_size: 1024
        send_batch_max_size: 4096
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_mib: 64
        limit_mib: 512
      groupbytrace: {}
      resource:
        attributes:
          - key: service.name
            from_attribute: service.name
            action: insert
          - key: k8s.container.name
            from_attribute: k8s.container.name
            action: insert
          - key: k8s.namespace.name
            from_attribute: k8s.namespace.name
            action: insert
          - key: k8s.pod.name
            from_attribute: k8s.pod.name
            action: insert
          - key: k8s.node.name
            from_attribute: k8s.node.name
            action: insert
          - key: host.name
            from_attribute: host.name
            action: insert
          - key: service.name
            from_attribute: service.name
            action: upsert
          - key: service.namespace
            from_attribute: service.namespace
            action: upsert
          - key: cluster.name
            value: "clustername"
            action: insert
      resourcedetection:
        detectors: [env, system]
        override: false
        timeout: 2s
      transform:
        log_statements:
          - context: log
            statements:
              - set(attributes["processing.note"], "Log enriched with resource and trace context")
              - set(attributes["service.name"], resource.attributes["service.name"])
              - set(attributes["k8s.pod.name"], resource.attributes["k8s.pod.name"])
              - set(attributes["k8s.node.name"], resource.attributes["k8s.node.name"])
              - set(attributes["host.name"], resource.attributes["host.name"])
              - set(attributes["cluster.name"], resource.attributes["cluster.name"])
              - set(attributes["trace_id"], trace_id)
              - set(attributes["span_id"], span_id)
      metricstransform:
        transforms:
          - include: system.cpu.usage
            action: update
            operations:
              - action: add_label
                new_label: host.name
                new_value: host.name
              - action: add_label
                new_label: cluster.name
                new_value: clustername
          - include: system.memory.usage
            action: update
            operations:
              - action: add_label
                new_label: host.name
                new_value: host.name
              - action: add_label
                new_label: cluster.name
                new_value: clustername

    exporters:
      debug:
        verbosity: basic
      otlphttp:
        endpoint: "http://loki-write.monitoring.svc.cluster.local:3100/otlp"
        tls:
          insecure: true
        headers:
          X-Scope-OrgID: tenant-1
      prometheus:
        endpoint: "0.0.0.0:8889"
        namespace: "monitoring"
        const_labels:
          k8s_cluster: "clustername"
        send_timestamps: true
        enable_open_metrics: true
      otlp/jaeger:
        endpoint: "http://jaeger-collector.monitoring.svc.cluster.local:4317"
        tls:
          insecure: true
        headers:
          "X-Tenant-ID": "default"

    extensions:
      health_check:
        endpoint: "0.0.0.0:13133"
        path: "/health"
      pprof:
        endpoint: "0.0.0.0:1777"
      zpages:
        endpoint: "0.0.0.0:55679"

    service:
      telemetry:
        metrics_address: "0.0.0.0:8888"
        metrics:
          level: detailed
          readers: 
             - pull: 
                exporter:
                  prometheus:
                    host: 0.0.0.0 
                    port: 9464
        logs:
          level: debug
      extensions: [health_check, pprof, zpages]
      pipelines:
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, resourcedetection, resource, metricstransform, batch]
          exporters: [debug, prometheus]
        traces:
          receivers: [otlp, jaeger]
          processors: [memory_limiter, resourcedetection, resource, groupbytrace, batch]
          exporters: [debug, otlp/jaeger]
        logs:
          receivers: [otlp, loki]
          processors: [memory_limiter, resourcedetection, resource, transform, batch]
          exporters: [debug, otlphttp]
