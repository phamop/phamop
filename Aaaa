Observability Architecture Proposals (AKS + Azure)

Scope: AKS workloads plus Azure PaaS (Logic Apps, Function Apps, App Services) using OSS tools: OpenTelemetry, OTel Collector, Prometheus, Loki, Jaeger, Grafana.

Executive Summary
We have two viable paths to centralize monitoring and visualization after path-rewrite issues through APIM impacted Grafana access:
	Option A — Centralized VM Stack (current draft): Move the full OSS stack (OTel Collector, Prometheus, Loki, Jaeger, Grafana) to an Azure VM gateway tier. All apps and clusters push to the VM’s collector and backends. 
NOTE: Simple topology but introduces a single point of failure - SPOF unless you build HA around the VM and storage.
	Option B — Distributed Data Planes + Central Grafana (recommended): Keep data-plane components (collectors and storage backends) in each cluster, securely expose them via TLS/mTLS Ingress and authenticate with Entra ID. 
	Run Grafana on an external VM only as a query/alert control plane that connects to each datasource. Improves resilience (no central SPOF), keeps data at source, and scales horizontally.
Note: I recommend Option B. It minimizes blast radius, avoids data loss if the VM/Grafana is down, and reduces cross-tenant/network chokepoints.

Non Functional Requirements (NFRs)
	Availability: Target 99.9% for visualization plane, 99.5%+ for data-plane per cluster, no single regional SPOF.
	Security: mTLS for telemetry ingestion, TLS 1.2+ everywhere; OIDC/OAuth2 (Azure Entra ID) SSO; least-privilege; private networking/peering; per-tenant tokens in Loki/Prometheus where applicable.
	Performance: p50 < 200ms query latency intra VNet; p95 < 800ms cross cluster queries; ingestion up to N events/sec (size by environment below).
	Retention: Metrics 7-15 days hot; Logs 7-14 days hot; Traces 3-7 days hot (sampling 5-10% default). Long-term is optional via archive/export.
	Operability: configs (Helm/Kustomize), IaC for VM + networking; dashboards & alerts provisioned as code; health probes & SLOs.
	Cost: Optimize egress by local writes; use SSD for WAL/TSDB and blob for chunks/indices where feasible.
Current Constraints & Context
	APIM path rewrite and layering broke Grafana UI and some plugin endpoints.
	Need to visualize multi-source data: AKS clusters, Logic Apps, Function Apps, App Services, etc.
	Initial all-in-one cluster approach worked functionally but suffered from upstream routing transformations.

Audience

Option A — Centralized VM Observability Stack (Current Proposal)
High-Level Diagram
High-Level Flow details
------------------------------
AKS Applications generate telemetry (instrumented via OpenTelemetry SDKs, Opentelemetry annotations or sidecars).

 OpenTelemetry Auto instrumentation  push data VM’s OTel Collector  Endpoints

VM OTel Collector distributes data to the respective exporter in the vm:

Metrics → Prometheus (scraped or pushed)

Logs → Loki (HTTP push)

Traces → Jaeger (gRPC push)

Grafana queries all backends (Loki, Prometheus, Jaeger) for visualization.


Apps & PaaS → Opentelemetry AutoInstrumentation (optional)→ VM OTel Collector 
    → Prometheus (metrics) → Grafana
    → Loki (logs)          → Grafana
    → Jaeger (traces)      → Grafana
Components
	Azure VM (Observability Component & Backends):
	OTel Collector 
	Prometheus (scrape & remote_write endpoints)
	Loki (log ingestion)
	Jaeger (collector + query + UI)
	Grafana (dashboards/alerts)
Installed Components on VM
Implementation
Component	Role	Data Type	Port
Loki	Log Aggregation & Storage	Logs	3100
Prometheus	Metrics and Storage	Metrics	9090
Jaeger	Distributed Tracing	Traces	14268, 16686
Grafana Dashboard	Data Visualization 	UI	3000
Opentelemetry Collector	Central Telemetry 	Log/Metrics/Traces	4317, 4318



Installed Components on Cluster
Instrumentation: OpenTelemetry Operator ( OpenTelemetry instrumentation CRD, Opentelemetry Annotation and Sidecar)
Sizing & Ports (initial)
	VM sizes: Medium (D8s_v4) or Large (D16s_v4) depending on environment.
	Ports (inbound): 4317/4318 (OTLP) – AKS to VM (Egress – AKS outbound)
	Inbound (within OS): 9090 (Prometheus), 3100 (Loki), 14268 & 16686 (Jaeger), 3000/443(Grafana - DNS).
	Disk layout: (1TB - Total Disk Request)
	/data/loki: 200-500 GB SSD, 7-14d
	/data/prom: 50-150 GB SSD, 7-15d
	/data/jaeger: 20-50 GB SSD, 3-7d
	/data/grafana: 20-50 GB
o	
Network & Security
	Same VNET or peered; dedicated subnet and NSG; private DNS.
	TLS encryption for end to end communication
	Grafana OIDC with Entra ID.
	Hardened OS baseline

 
Pros
1.	Single, simple landing zone for ingestion and querying.
2.	Minimal change to app/cluster agents (single OTLP target).
3.	Easier to grant external teams read only Grafana access.
Cons / Risks
1. SPOF risk: If the VM stack fails, all ingestion and query can be impacted.
2. Horizontal scale requires sharding and multiple VMs (more work).
3. Higher cross network dependency and potential backpressure under spikes.
4. Local TSDB/indices can be a bottleneck without careful I/O tuning and backups.
5. Grafana downtime removes visibility and alert evaluation if run on same host.




Cost

Compute Costs
VM Size	vCPUs	RAM		Cost (CAD/month) $	Use Case
Medium - Standard_D8s_v4	8	32GB	430.71	POC (Dev/Test)
Large - Standard_D16s_v4	16	64GB	861.43	POC (Dev/Test)

Storage Costs (Premium SSD)
Disk Type	Size (GB)	Cost (CAD/month) $	Purpose
OS Disk (P10)	128	22.17	Boot volume/OS
Data Disk (P20)	512	100.93	Loki (logs)
Data Disk (P15)	256	52.41	Prometheus (metrics)
Data Disk (P15)	256	52.41	Jaeger (traces)
Data Disk (P10)	128	22.17	Grafana (dashboards)
Total Storage (1.25TB)	1250 	260.09	All

Network Traffic Costs
Traffic Type	Data Volume	Cost (CAD/month) $
Inbound	Free	0.0
Outbound	First 5GB	0.0
Additional Outbound	0.087/GB	8.70 (100GB estimated)
Assumptions:
100GB/month outbound (AKS → VM telemetry).
Inbound traffic is always free in Azure.

Total Estimated Monthly Cost
Component	Medium VM (D8s_v4) $	Large VM (D16s_v4) $
Compute	430.71	861.43
Storage	260.09	260.09
Network	8.70	8.70
Total
	699.50	1130.22

		

Access the Azure pricing estimate for this solution via the calculator link below:

Estimatedfolder
 

HA Enhancements (if you proceed with A)
1. VMs behind an Azure Load Balancer for OTLP 4317/4318.
 Receive all metrics, logs, traces from AKS via OTLP.

Run Grafana, Prometheus+Thanos, Loki (distributed), Jaeger (collectors/query) and an OTel gateway on every VM (Docker).

Use Premium SSD for fast local TSDB/WAL/caches; use Azure Blob for durable, scalable object storage.

Scale from 3 → 10 VMs without client reconfiguration.

Provide TLS/mTLS, SSO, least-privilege access, and safe concurrency (no data corruption).


Integration Flow
Azure Components and Tasks
AKS (Workloads + OTel Agent/Sidecars)
   │ OTLP:gRPC/HTTP (4317/4318)
   ▼
Azure Standard Internal Load Balancer (ILB, single VIP)
   ├─ Health probe: 4317 (OTLP gRPC), 4318 (HTTP)
   ▼
Azure VMSS (min=3, max=10) – each VM runs Docker:
   • nginx (TLS/route UI if not using AppGW)
   • otel-collector (gateway)  ← scrapes to Prom exporter :9464
   • prometheus  + thanos-sidecar (local TSDB on Premium SSD)
   • thanos-store + thanos-query (across all shards)
   • loki (distributor/ingester/querier/query-frontend/compactor)
   • jaeger-collector + jaeger-query (UI)
   • grafana (SSO via Entra ID; uses shared Postgres DB)
Storage:
   • Premium SSD (/data):
       - /data/prometheus, /data/loki-cache, /data/thanos-cache
   • Azure Blob:
       - thanos-bucket (metrics, immutable blocks)
       - loki-bucket (chunks + index via boltdb-shipper)
   • Traces: Elastic Cloud on Azure (recommended) or OpenSearch VMSS
UIs:
   • App Gateway (WAF) → nginx (or direct) → Grafana/Jaeger
Security:
   • NSGs (strict), TLS/mTLS, Key Vault, Managed Identity


Costs:
Setting VMSS setup, to handle autoscaling 
Component	Specification	Quantity	Monthly Cost($)
VM Compute (D8s_v5)	8 vCPUs 32GB	3(min)	1,292.14
App Gateway 	WAF	1	434.74
Premium Disk	P20	1(1TB)	183.0
Standard Load Balancer	(Standard)	1	32.05
Blob Storage (hot tiers)	Retention (0.0208)/GB-month	2(TB)	50
Total/Monthly			1991.93
Azure Load Balancer
Component	Specification	Quantity	Price/Hour ($)	Monthly Cost ($)
Standard Load Balancer	Public + Load Balancing Rules	1	Base monthly	30 + data Processing
Data Processing	Estimated 1TB/Month	1TB	0.007/GB	5.00
Load Balancer Subtotal				35
Storage Costs (Premium SSD)
Disk Type	Size (GB)	Cost (CAD/month) $	Purpose
OS Disk (P10)	128	22.17	Boot volume/OS
Data Disk (P20)	512	100.93	Loki (logs)
Data Disk (P15)	256	52.41	Prometheus (metrics)
Data Disk (P15)	256	52.41	Jaeger (traces)
Data Disk (P10)	128	22.17	Grafana (dashboards)
Total Storage (1.25TB)	1250 	260.09	1 VM
2 VM (2.5TB)	2500	520.18	2 VM

Network Traffic Costs
Traffic Type	Data Volume	Cost (CAD/month) $
Inbound	Free	0.0
Outbound	First 5GB	0.0
Additional Outbound	0.087/GB	43.07 (500GB estimated)

Total Estimated Monthly Cost
Component	Medium VM (D8s_v4) $/Month	Large VM (D16s_v4) $/Month
Compute	861.42	1722.86
Storage	520.18	520.18
Network	43.07 (500GB estimated)	43.07 (500GB estimated)
Azure Load Balancer	35.00	35.00
Total
	1455.67	2317.11

 

Option B — Distributed Data Planes + Central Grafana (Recommended) is 

High-Level Diagram
Cluster A   Cluster B   PaaS Apps
  │           │            │
 [OTel]     [OTel]      [OTel]
  │           │            │
[Prom]     [Prom]      (OTLP → cluster collector)
[Loki ]    [Loki ]
[Jaeger]   [Jaeger]
  │           │
  └── TLS/mTLS Ingress (private) ──► Grafana (VM, HA) ◄── other data sources (e.g., Azure Monitor, CI/CD)
Key Principles
	Keep data where it is produced. Each cluster runs its own OTel Collector, Prometheus, Loki, and Jaeger with appropriate retention.
	Expose only query endpoints (Prometheus /api/v1, Loki /loki/api, Jaeger query) via Ingress with TLS and mTLS from Grafana to the data sources.
	Centralize only the control plane (Grafana) on a VM (or small VM set) for dashboards, alert rules, and SSO.
Secure Exposure Pattern
1. Ingress Controller: NGINX Ingress Controller without path rewrites for data sources.
2. Certificates: cert manager issuing per service certs; client certs for Grafana; TLS 1.3.
3. Auth:
	Grafana SSO with Azure Entra ID (Teams/Groups for RBAC).
	Data source protection: mTLS + IP allowlist (VNet peering) + optional basic/OAuth2 via oauth2 proxy at the ingress layer.
4. Networking:
	Private IPs only for backends; Grafana VM in same VNET/peered VNets; Private DNS A records per service.
	Bypass APIM for Grafana and data-source paths to avoid rewrite issues.
o	
Implementation Details
1. Grafana VM:
	2 small/medium VMs or a VMSS (HA) with PostgreSQL as the Grafana database; dashboards & datasources provisioned as code.
2. Prometheus per cluster:
	Scrape in cluster; 7-15d retention.
	Optional: federation or remote_write to a long term store later (kept out of scope for now to stay OSS pure).
3. Loki per cluster:
	Ingest via Promtail/OTel; 7-14d retention.
	Optional: use Azure Blob for chunks to reduce node SSD pressure.
4. Jaeger per cluster:
	OTel→Jaeger collector; 3-7d hot storage with sampling 5-10%.
	Query endpoint exposed via Ingress for Grafana plugin.
5. OTel Collector:
	In-cluster Deployment (preferred)  + app sidecars where necessary.
	Processors: batch, memory_limiter, resource detection; exporters: prometheusremotewrite, loki, jaeger.
Pros
1. Resilience: Loss of Grafana does not impact ingestion or data retention.
2. No central SPOF: Each cluster continues to collect and store locally.
3. Scalability: Add clusters without re architecting the center; per cluster SLOs.
4. Security: Minimal surface; mTLS and private endpoints; avoids APIM rewrite issues entirely.
5. Cost control: Limits cross network egress; right size per cluster.
Cons / Trade offs
1. Multiple datasources to manage in Grafana (solved by provisioning-as-code).
2. Cross cluster panels can have higher query fan out latency.
3. Need consistent schema/labels across clusters for clean dashboards.

Access & AuthZ Strategy
1. Grafana: Entra ID OIDC; map groups to folders/teams; disable local sign ups; enforce MFA via Entra policy.
2. Data sources:
a)	mTLS client certs (Grafana→datasource).
b)	Per cluster API tokens (Loki/Prometheus) with least privilege scopes.
c)	Network ACLs restrict to Grafana VM subnet.

Storage & Retention
1. Keep hot retention small (7-15 days) per cluster; push long term to archive later if needed.
2. Filesystems with node local SSDs for WAL; consider blob/object storage for chunks/indices to reduce risk on node failover.
HA, DR, and Backups
1. Grafana:
a)	2 instances behind NLB; PostgreSQL single AZ with PITR or Flexible Server HA.
b)	Nightly backup of dashboards (JSON) + provisioning repos.
2. Per cluster backends:
a)	Prometheus: double replica; PVC with fast SSD; snapshot to Azure Files weekly.
b)	Loki: min 3 replicas for querier/ingester in prod; WAL on SSD; chunks to blob (optional).
c)	Jaeger: at least 2 collectors; consider external storage if trace volume grows.
3. Disaster recovery: Rehydrate Grafana from Git + DB backup; clusters continue operating independently.
Networking Reference
1. Avoid APIM for Grafana and data-source paths. Use dedicated DNS if possible
2. Private DNS zone linked to VNets; no public exposure.
3. NSG rules: allow only required ports from Grafana subnet to cluster nodepools.
4. ACL :

Rollout Plan
1. Foundations: Create VNET peerings, subnets, NSGs, Private DNS zones.
2. Grafana VM: Provision HA pair; attach PostgreSQL; configure Entra ID SSO; enable backup.
3. Cluster Baseline: Ensure OTel Collector, Prometheus, Loki, Jaeger running with desired retention; standardize labels.
4. Ingress Security: Issue certs; apply Ingress for Prom/Loki/Jaeger with mTLS; validate from Grafana VM.
5. Datasource as Code: Commit Grafana datasources.yml and folder/permissions; import curated dashboards.
6. Cutover: Point users to Grafana VM URL; decommission APIM path to old Grafana; monitor SLOs.
7. Hardening & Scale: Add rate limits, alert noise reduction, and perenv RBAC.


Costs

Component	Small Grafana VM(D2s_v3)$/Month	Roomier Grafana VM(D4s_v4)$/Month
Compute	111.7	215.36
Specification	2 vCPUs, 8GB RAM	4 vCPUs, 16GB RAM

OS Disk (P4: 32GB)	7.28	7.28
Network(same VNet/AZ)	6.55	6.55
Total
	125.53	229.18

Cost Considerations
1. Option A: Larger VM(s), higher SSD, and ops for HA; simpler per cluster footprint.
2. Option B: Modest Grafana VM(s) + storage per cluster; reduced egress; scales linearly with number of clusters.
Risk Register & Mitigations
	Path rewrite regression: Bypass APIM, test SSO and callbacks explicitly.
	Query fan out latency: Use dashboard variables to filter scope, set sensible panel timeouts.
	Label cardinality explosion: Enforce label hygiene in OTel pipelines and scrape configs.
	Trace volume spikes: Apply tail based sampling in OTel Collector.
	Secrets handling: KMS/Key Vault integration, short lived tokens, cert rotation policy.

Final Recommendation
Adopt Option B (Distributed Data Planes + Central Grafana). It prevents a central SPOF, keeps ingestion resilient, and aligns with security best practices (mTLS, private networking, SSO). Reserve Option A for constrained environments or interim phases, and only with explicit HA additions (ALB, storage externalization, and split roles).
 
